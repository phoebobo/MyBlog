{
  "filename": "数据分析.md",
  "__html": "<h1>数据分析</h1>\n<h2>1、numpy：多维数组的创建</h2>\n<p>多维数组（矩阵ndarray）</p>\n<p>ndarray的基本属性</p>\n<pre><code>shape 维度的大小\n\nndim维度的个数\n\ndtype数据类型\n</code></pre>\n<h3>1.1 随机抽样创建</h3>\n<h4>1.1.1 rand</h4>\n<p>生成指定维度的随机多维度浮点型数组,区间范围是[0,1)</p>\n<pre><code class=\"language-python\">Create an array of the given shape <span class=\"hljs-keyword\">and</span> populate it <span class=\"hljs-keyword\">with</span>\n            random samples <span class=\"hljs-keyword\">from</span> a uniform distribution\n            over ``[<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1</span>)``.\n            \nnd1 = np.random.rand(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>)\nprint(nd1)\nprint(<span class=\"hljs-string\">'维度的个数'</span>,nd1.ndim)\nprint(<span class=\"hljs-string\">'维度的大小'</span>,nd1.shape)\nprint(<span class=\"hljs-string\">'数据类型'</span>,nd1.dtype)  <span class=\"hljs-comment\">#float 64</span>\n\n                           \n\n</code></pre>\n<pre><code>展示\n[[[0.6692067  0.2720613  0.51154113 0.82428366 0.46295541]\n  [0.90008085 0.32678138 0.22799996 0.01463876 0.91932592]\n  [0.94994095 0.1005888  0.97856803 0.95835044 0.4232734 ]\n  [0.55823696 0.67435857 0.80571127 0.31125564 0.51347285]]\n\n [[0.79288383 0.87991494 0.36959603 0.83993517 0.44854427]\n  [0.29233904 0.21511221 0.23838737 0.31218621 0.01570319]\n  [0.63118773 0.6943842  0.42748468 0.55841017 0.58764804]\n  [0.65398815 0.58153545 0.57424707 0.49788028 0.54942051]]\n\n [[0.9069376  0.4375912  0.12404622 0.73877842 0.91480335]\n  [0.37166892 0.13312303 0.12471981 0.8086709  0.72126696]\n  [0.93126097 0.11578659 0.82806954 0.91416224 0.93896591]\n  [0.78597169 0.46466087 0.0921524  0.11408107 0.15356255]]]\n维度的个数 3\n维度的大小 (3, 4, 5)\n数据类型 float64\n###################\n</code></pre>\n<h4>1.1.2 uniform</h4>\n<pre><code class=\"language-python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">uniform</span><span class=\"hljs-params\">(low=<span class=\"hljs-number\">0.0</span>, high=<span class=\"hljs-number\">1.0</span>, size=None)</span>:</span> <span class=\"hljs-comment\"># real signature unknown; restored from __doc__</span>\n    <span class=\"hljs-string\">\"\"\"\n    uniform(low=0.0, high=1.0, size=None)\n    \n            Draw samples from a uniform distribution.\n    \n            Samples are uniformly distributed over the half-open interval\n            ``[low, high)`` (includes low, but excludes high).  In other words,\n            any value within the given interval is equally likely to be drawn\n            by `uniform`.\n    \n            Parameters\n            ----------\n            low : float or array_like of floats, optional\n                Lower boundary of the output interval.  All values generated will be\n                greater than or equal to low.  The default value is 0.\n            high : float or array_like of floats\n                Upper boundary of the output interval.  All values generated will be\n                less than high.  The default value is 1.0.\n            size : int or tuple of ints, optional\n                Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n                ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n                a single value is returned if ``low`` and ``high`` are both scalars.\n                Otherwise, ``np.broadcast(low, high).size`` samples are drawn.\n    \n            Returns\n            -------\n            out : ndarray or scalar\n                Drawn samples from the parameterized uniform distribution.\n    \n            See Also\n            --------\n            randint : Discrete uniform distribution, yielding integers.\n            random_integers : Discrete uniform distribution over the closed\n                              interval ``[low, high]``.\n            random_sample : Floats uniformly distributed over ``[0, 1)``.\n            random : Alias for `random_sample`.\n            rand : Convenience function that accepts dimensions as input, e.g.,\n                   ``rand(2,2)`` would generate a 2-by-2 array of floats,\n                   uniformly distributed over ``[0, 1)``.\n    \n            Notes\n            -----\n            The probability density function of the uniform distribution is\n    \n            .. math:: p(x) = \\frac{1}{b - a}\n    \n            anywhere within the interval ``[a, b)``, and zero elsewhere.\n    \n            When ``high`` == ``low``, values of ``low`` will be returned.\n            If ``high`` &lt; ``low``, the results are officially undefined\n            and may eventually raise an error, i.e. do not rely on this\n            function to behave when passed arguments satisfying that\n            inequality condition.\n    \n            Examples\n            --------\n            Draw samples from the distribution:\n    \n            &gt;&gt;&gt; s = np.random.uniform(-1,0,1000)\n    \n            All values are within the given interval:\n    \n            &gt;&gt;&gt; np.all(s &gt;= -1)\n            True\n            &gt;&gt;&gt; np.all(s &lt; 0)\n            True\n    \n            Display the histogram of the samples, along with the\n            probability density function:\n    \n            &gt;&gt;&gt; import matplotlib.pyplot as plt\n            &gt;&gt;&gt; count, bins, ignored = plt.hist(s, 15, normed=True)\n            &gt;&gt;&gt; plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n            &gt;&gt;&gt; plt.show()\n    \"\"\"</span>\n    <span class=\"hljs-keyword\">pass</span>\n</code></pre>\n<pre><code>nd = np.random.uniform(2)\nnd2 = np.random.uniform(-1,5,size=(2,3))\nprint(nd2)\nprint(type(nd2))\nprint(nd2.ndim)\nprint(nd2.dtype)\nprint(nd2.shape)\n展示：\n[[3.59873373 2.60052091 0.45326737]\n [2.87653583 2.15374128 3.38291766]]\n&lt;class 'numpy.ndarray'&gt;\n2\nfloat64\n(2, 3)\n\n</code></pre>\n<h4>1.1.3 randint</h4>\n<pre><code>def randint(low, high=None, size=None, dtype='l'): # real signature unknown; restored from __doc__\n    &quot;&quot;&quot;\n    randint(low, high=None, size=None, dtype='l')\n    \n            Return random integers from `low` (inclusive) to `high` (exclusive).\n    \n            Return random integers from the &quot;discrete uniform&quot; distribution of\n            the specified dtype in the &quot;half-open&quot; interval [`low`, `high`). If\n            `high` is None (the default), then results are from [0, `low`).\n    \n            Parameters\n            ----------\n            low : int\n                Lowest (signed) integer to be drawn from the distribution (unless\n                ``high=None``, in which case this parameter is one above the\n                *highest* such integer).\n            high : int, optional\n                If provided, one above the largest (signed) integer to be drawn\n                from the distribution (see above for behavior if ``high=None``).\n            size : int or tuple of ints, optional\n                Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n                ``m * n * k`` samples are drawn.  Default is None, in which case a\n                single value is returned.\n            dtype : dtype, optional\n                Desired dtype of the result. All dtypes are determined by their\n                name, i.e., 'int64', 'int', etc, so byteorder is not available\n                and a specific precision may have different C types depending\n                on the platform. The default value is 'np.int'.\n    \n                .. versionadded:: 1.11.0\n    \n            Returns\n            -------\n            out : int or ndarray of ints\n                `size`-shaped array of random integers from the appropriate\n                distribution, or a single such random int if `size` not provided.\n    \n            See Also\n            --------\n            random.random_integers : similar to `randint`, only for the closed\n                interval [`low`, `high`], and 1 is the lowest value if `high` is\n                omitted. In particular, this other one is the one to use to generate\n                uniformly distributed discrete non-integers.\n    \n            Examples\n            --------\n            &gt;&gt;&gt; np.random.randint(2, size=10)\n            array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n            &gt;&gt;&gt; np.random.randint(1, size=10)\n            array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n    \n            Generate a 2 x 4 array of ints between 0 and 4, inclusive:\n    \n            &gt;&gt;&gt; np.random.randint(5, size=(2, 4))\n            array([[4, 0, 2, 1],\n                   [3, 2, 2, 0]])\n    &quot;&quot;&quot;\n    pass\n</code></pre>\n<pre><code>print('############################')\nnd3 = np.random.randint(1,20,size=(3,4))\nprint(nd3)\n\n展示：\n############################\n[[ 4 15 10  5]\n [11 11  2  4]\n [ 6  1 16 19]]\n\n注意点：\n1、如果没有指定最大值，只是指定了最小值，范围是[0,最小值)\n2、如果有最小值，也有最大值[最小值，最大值)\n\n</code></pre>\n<h3>1.2 序列创建</h3>\n<h4>1.2.1 array</h4>\n<pre><code>通过列表进行创建\nnd4 = np.array([1,2,3])\n[1 2 3]\n\n通过列表嵌套列表创建\nnd5 = np.array([[1,2,3],[4,5]])\n[list([1, 2, 3]) list([4, 5])]\n\n综合\nnd4 = np.array([1,2,3])\nnd5 = np.array([[1,2,3],[4,5,6]])\nprint(nd4)\nprint(nd4.ndim)\nprint(nd4.shape)\nprint(nd5)\nprint(nd5.ndim)\nprint(nd5.shape)\n\n[1 2 3]\n1\n(3,)\n[[1 2 3]\n [4 5 6]]\n2\n(2, 3)\n</code></pre>\n<h4>1.2.2 zeros</h4>\n<pre><code class=\"language-python\">\nprint(np.zeros((<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">4</span>)))\n[[<span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span>]\n [<span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span>]\n [<span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span>]\n [<span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span> <span class=\"hljs-number\">0.</span>]]\n注意点：\n<span class=\"hljs-number\">1</span>、创建的数组里面的数据为<span class=\"hljs-number\">0</span>\n<span class=\"hljs-number\">2</span>、默认的数据类型是float\n<span class=\"hljs-number\">3</span>、可以指定其他的数据类型\n</code></pre>\n<h4>1.2.3 ones</h4>\n<h4>1.2.4 arange</h4>\n<pre><code class=\"language-python\">nd6 = np.arange(<span class=\"hljs-number\">10</span>)\nprint(nd6)\nnd7 = np.arange(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">10</span>)\nprint(nd7)\nnd8 = np.arange(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">2</span>)\nprint(nd8)\n\n展示\n<span class=\"hljs-comment\">##################</span>\n[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">9</span>]\n[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">9</span>]\n[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">8</span>]\n\n注意点：\n<span class=\"hljs-number\">1</span>、只填写一位数[<span class=\"hljs-number\">0</span>，这位)\n<span class=\"hljs-number\">2</span>、填写两位[最低位，最高位)\n<span class=\"hljs-number\">3</span>、填写<span class=\"hljs-number\">3</span>位数[最低位，高位，步长）\n<span class=\"hljs-number\">4</span>、创建的是一维数组\n<span class=\"hljs-number\">5</span>、等同于np.array(range())\n</code></pre>\n<h3>1.3 数组重新排列</h3>\n<pre><code class=\"language-python\">print(<span class=\"hljs-string\">'##########################'</span>)\nnd9 = np.arange(<span class=\"hljs-number\">10</span>)\nprint(nd9)\nnd10 = nd9.reshape(<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">5</span>)\nprint(nd10)\nprint(nd9)\n\n展示：\n<span class=\"hljs-comment\">##########################</span>\n[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">9</span>]\n[[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span>]\n [<span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">9</span>]]\n[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">9</span>]\n\n注意点：\n<span class=\"hljs-number\">1</span>、有返回值，返回新的数组，原始数组不受影响\n<span class=\"hljs-number\">2</span>、进行维度大小的设置的过程中，要注意数据的个数，注意元素的个数\n\n</code></pre>\n<pre><code class=\"language-python\">print(<span class=\"hljs-string\">'###################'</span>)\nnd11 = np.arange(<span class=\"hljs-number\">10</span>)\nprint(nd11)\nnd12 = np.random.shuffle(nd11)\nprint(nd12)\nprint(nd11)\n展示\n<span class=\"hljs-comment\">###################</span>\n[<span class=\"hljs-number\">0</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">9</span>]\n<span class=\"hljs-literal\">None</span>\n[<span class=\"hljs-number\">5</span> <span class=\"hljs-number\">6</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-number\">8</span> <span class=\"hljs-number\">7</span> <span class=\"hljs-number\">9</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">0</span> <span class=\"hljs-number\">3</span>]\n\n注意点：\n<span class=\"hljs-number\">1</span>、在原始数据集上做的操作\n<span class=\"hljs-number\">2</span>、将原始数组的元素进行重新排列，打乱顺序\n<span class=\"hljs-number\">3</span>、shuffle这个没有返回值\n</code></pre>\n<p>两个可以配合使用，先打乱，再重新排列</p>\n<h3>1.4 数据类型的转换</h3>\n<pre><code class=\"language-python\">print(<span class=\"hljs-string\">'########################'</span>)\nnd13 = np.arange(<span class=\"hljs-number\">10</span>,dtype=np.int64)\nprint(nd13)\nnd14 = nd13.astype(np.float64)\nprint(nd14)\nprint(nd13)\n\n\n注意点：\n<span class=\"hljs-number\">1</span>、astype（）不在原始数组做操作，有返回值，返回的是更改数据类型的新数组\n<span class=\"hljs-number\">2</span>、在创建新数组的过程中，有dtype参数进行指定\n</code></pre>\n<h3>1.5 数组转列表</h3>\n<pre><code>print(list(arr1))\nprint(arr1.tolist())\n</code></pre>\n<h2>2、numpy的矩阵计算</h2>\n<h3>2.1 一维数组与一维数组</h3>\n<pre><code class=\"language-python\">arr1 = np.arange(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">6</span>)\narr2 = np.array([<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">20</span>,<span class=\"hljs-number\">30</span>,<span class=\"hljs-number\">40</span>,<span class=\"hljs-number\">50</span>])\nprint(arr1)\nprint(arr2)\nprint(arr1+arr2)\n展示：\n[<span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">5</span>]\n[<span class=\"hljs-number\">10</span> <span class=\"hljs-number\">20</span> <span class=\"hljs-number\">30</span> <span class=\"hljs-number\">40</span> <span class=\"hljs-number\">50</span>]\n[<span class=\"hljs-number\">11</span> <span class=\"hljs-number\">22</span> <span class=\"hljs-number\">33</span> <span class=\"hljs-number\">44</span> <span class=\"hljs-number\">55</span>]\n注意点：\n<span class=\"hljs-number\">1</span>、一维数组的元素个数是相同的，不然无法完成广播\n<span class=\"hljs-number\">2</span>、按照数组对应的下标来进行算术运算，返回一个新的数组，同时保证数组的元素是一致的、\n\n\n</code></pre>\n<h3>2.2 一维数组与多维数组</h3>\n<pre><code class=\"language-python\">arr4 = arr2.reshape((<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">5</span>))\nprint(arr4)\n\nprint(<span class=\"hljs-string\">'相乘'</span>)\nprint(arr4*arr3)\nprint(<span class=\"hljs-string\">'与单独的一个数据'</span>)\nprint(arr4*<span class=\"hljs-number\">100.0</span>)\n\narr5 = np.array([<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">5</span>])\nprint(arr4+arr5)\n\n注意点：\n<span class=\"hljs-number\">1</span>、元素的个数必须相同（一维数组的元素个数必须与多维数组的列的元素个数一致的）\n</code></pre>\n<h3>2.3 多维与多维</h3>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\">#转多维数组</span>\narr3 = arr1.reshape((<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">5</span>))\nprint(arr3)\n\narr4 = arr2.reshape((<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">5</span>))\nprint(arr4)\n\nprint(<span class=\"hljs-string\">'相乘'</span>)\nprint(arr4*arr3)\nprint(<span class=\"hljs-string\">'与单独的一个数据'</span>)\n注意点：\n数组的维度大小必须是一致的\n</code></pre>\n<h3>2.4 数组与单独的一个数据</h3>\n<pre><code class=\"language-python\">print(<span class=\"hljs-string\">'与单独的一个数据'</span>)\nprint(arr4*<span class=\"hljs-number\">100.0</span>)\n</code></pre>\n<h2>3、索引与切片</h2>\n<h3>3.1 一维数组</h3>\n<pre><code>import numpy as np\n\narr1 = np.arange(10)\nprint('原始数组',arr1)\nprint('取单独的一个数据')\nprint(arr1[0])\nprint(arr1[-1])\nprint('取多个数据')\nprint(arr1[0:-1])\nprint(arr1[0:3])\n\n注意点：\n1、取单个数据，取下标，下标从0开始\n2、取多个数据，：隔开，左闭右开\n3、参考列表【起始位置：终点位置：步长】左闭右开\n</code></pre>\n<h3>3.2 多维数组</h3>\n<pre><code>print('#####取单独的一行######')\nprint(arr1[0])\nprint('#####取单独的一行中的一列（取单个元素）######')\nprint(arr1[0][-1])\nprint(arr1[0,-1])\n\narr1 = np.random.randint(0,99,(3,4))\nprint('原始数组',arr1)\nprint('######取单独的一列######')\nprint(arr1[:,2])\n展示：\n原始数组 [[24 65 37 70]\n [46 84 67 37]\n [55 68 75 15]]\n######取单独的一列######\n[37 67 75]\n\n\narr1 = np.random.randint(0,99,(3,4))\nprint('原始数组',arr1)\nprint('######取连续多行######')\nprint(arr1[0:2])\nprint('######取连续多行指定多列######')\nprint(arr1[0:2,0:2])\nprint('######取不连续多行######')\nprint(arr1[[0,2]])\nprint(arr1[[0,-1]])\n展示\n原始数组 [[ 1 16 77 15]\n [24 62 88 83]\n [60 50 52  4]]\n######取连续多行######\n[[ 1 16 77 15]\n [24 62 88 83]]\n######取连续多行指定多列######\n[[ 1 16]\n [24 62]]\n######取不连续多行######\n[[ 1 16 77 15]\n [60 50 52  4]]\n[[ 1 16 77 15]\n [60 50 52  4\n \n \narr1 = np.random.randint(0,99,(3,4))\nprint('原始数组',arr1)\nprint('######取连续多行，不连续多列######')\nprint(arr1[0:2,[0,-1]])\nprint('######取不连续多行，不连续多列######')\nprint(arr1[[0,2],[0,-1]])\nprint('**************')\nprint(arr1[[0,2]][:,[0,-1]])\n\n展示：\n原始数组 [[43 56  1 95]\n [43 89 76 16]\n [15 67 64 52]]\n######取连续多行，不连续多列######\n[[43 95]\n [43 16]]\n######取不连续多行，不连续多列######\n[43 52]\n**************\n[[43 95]\n [15 52]]\n\n\n</code></pre>\n<h3>3.3 条件索引</h3>\n<pre><code>arr1 = np.array([\n    [2010,2011,2012],\n    [2013,2014,2015],\n    [2019,2018,2020]\n])\nprint('原始数组',arr1)\nprint('##################')\nprint(arr1&gt;2013)\nprint(arr1[arr1&gt;20])\n展示：\n原始数组 [[2010 2011 2012]\n [2013 2014 2015]\n [2019 2018 2020]]\n##################\n[[False False False]\n [False  True  True]\n [ True  True  True]]\n[2010 2011 2012 2013 2014 2015 2019 2018 2020]\n注意点：\n返回所有条件为真的数据，组合成了一个一维数组\n\n\narr1 = np.array([\n    [2010,2011,2012],\n    [2013,2014,2015],\n    [2019,2018,2020]\n])\nprint('原始数组',arr1)\nprint('##################')\nprint(arr1&gt;2013)\nprint(arr1[arr1&gt;20])\nprint(arr1[(arr1&gt;2013) &amp; (arr1&lt;2019)])\nprint(arr1[(arr1&lt;=2013)|(arr1&gt;2019)])\nprint('#########取闰年##########')\n# 1、能被4整除但是不能被100整除2、能被400整除\nprint(arr1[((arr1%4==0)&amp;(arr1%100!=0))|(arr1%400==0)])\n\n</code></pre>\n<h2>4、通用函数（ufunc）</h2>\n<h3>4.1 一元计算函数</h3>\n<pre><code>import numpy as np\n\narr1 = np.random.uniform(-5,10,(3,4))\nprint(arr1)\nprint('向上取整')\nprint(np.ceil(arr1))\nprint('向下取整')\nprint(np.floor(arr1))\nprint('四舍五入')\nprint(np.rint(arr1))\nprint('取绝对值')\nprint(np.abs(arr1))\nprint('取反')\nprint(np.negative(arr1))\nprint(np.negative(-1))\nprint('平方')\nprint(np.square(arr1))\nprint('平方根')\nprint(np.sqrt(np.abs(arr1)))\nprint('分成小数与整数部分')\nprint(np.modf(arr1)[0])\nprint(np.modf(arr1)[1])\nprint('判断空值')\nprint(np.isnan(arr1))\n\n展示：\n[[-0.69587753  1.14875406  8.53686918  9.4968895 ]\n [ 3.68639285  6.79418179 -0.44578217 -3.71251504]\n [ 1.38537669  9.2594615  -4.54692681  8.64836814]]\n向上取整\n[[-0.  2.  9. 10.]\n [ 4.  7. -0. -3.]\n [ 2. 10. -4.  9.]]\n向下取整\n[[-1.  1.  8.  9.]\n [ 3.  6. -1. -4.]\n [ 1.  9. -5.  8.]]\n四舍五入\n[[-1.  1.  9.  9.]\n [ 4.  7. -0. -4.]\n [ 1.  9. -5.  9.]]\n取绝对值\n[[0.69587753 1.14875406 8.53686918 9.4968895 ]\n [3.68639285 6.79418179 0.44578217 3.71251504]\n [1.38537669 9.2594615  4.54692681 8.64836814]]\n取反\n[[ 0.69587753 -1.14875406 -8.53686918 -9.4968895 ]\n [-3.68639285 -6.79418179  0.44578217  3.71251504]\n [-1.38537669 -9.2594615   4.54692681 -8.64836814]]\n1\n平方\n[[ 0.48424553  1.31963589 72.87813532 90.19091012]\n [13.58949223 46.16090626  0.19872174 13.78276796]\n [ 1.91926856 85.73762719 20.67454338 74.79427144]]\n平方根\n[[0.83419274 1.07179945 2.92179212 3.08170237]\n [1.91999814 2.60656513 0.6676692  1.92678879]\n [1.17702026 3.04293633 2.13235241 2.9408108 ]]\n分成小数与整数部分\n[[-0.69587753  0.14875406  0.53686918  0.4968895 ]\n [ 0.68639285  0.79418179 -0.44578217 -0.71251504]\n [ 0.38537669  0.2594615  -0.54692681  0.64836814]]\n[[-0.  1.  8.  9.]\n [ 3.  6. -0. -3.]\n [ 1.  9. -4.  8.]]\n判断空值\n[[False False False False]\n [False False False False]\n [False False False False]]\n \n 注意点：\n isnan这个函数，一般配合条件索引\n</code></pre>\n<h3>4.2 二元计算函数</h3>\n<pre><code>import numpy as np\n\n# arr1 = np.random.uniform(-5,10,(3,4))\n# print(arr1)\n# print('向上取整')\n# print(np.ceil(arr1))\n# print('向下取整')\n# print(np.floor(arr1))\n# print('四舍五入')\n# print(np.rint(arr1))\n# print('取绝对值')\n# print(np.abs(arr1))\n# print('取反')\n# print(np.negative(arr1))\n# print(np.negative(-1))\n# print('平方')\n# print(np.square(arr1))\n# print('平方根')\n# print(np.sqrt(np.abs(arr1)))\n# print('分成小数与整数部分')\n# print(np.modf(arr1)[0])\n# print(np.modf(arr1)[1])\n# print('判断空值')\n# print(np.isnan(arr1))\n\narr1 = np.arange(10).reshape((2,5))\narr2 = np.arange(10,20).reshape((2,5))\nprint('arr1',arr1)\nprint('arr2',arr2)\nprint('两个数组之间的相加操作')\nprint(np.add(arr1,arr2))\nprint('两个数组之间的相减')\nprint(np.subtract(arr1,arr2))\nprint('两个数组之间相除')\nprint(np.divide(arr1,arr2))\nprint(np.floor_divide(arr1,arr2)) #取整\nprint(np.mod(arr1,arr2)) #取余\nprint('元素相乘')\nprint(np.multiply(arr1,arr2))\n\n展示：\narr1 [[0 1 2 3 4]\n [5 6 7 8 9]]\narr2 [[10 11 12 13 14]\n [15 16 17 18 19]]\n两个数组之间的相加操作\n[[10 12 14 16 18]\n [20 22 24 26 28]]\n两个数组之间的相减\n[[-10 -10 -10 -10 -10]\n [-10 -10 -10 -10 -10]]\n两个数组之间相除\n[[0.         0.09090909 0.16666667 0.23076923 0.28571429]\n [0.33333333 0.375      0.41176471 0.44444444 0.47368421]]\n[[0 0 0 0 0]\n [0 0 0 0 0]]\n[[0 1 2 3 4]\n [5 6 7 8 9]]\n元素相乘\n[[  0  11  24  39  56]\n [ 75  96 119 144 171]]\n</code></pre>\n<h3>4.3 三元计算函数</h3>\n<pre><code>arr1 = np.arange(10).reshape((2,5))\narr2 = np.arange(10,20).reshape((2,5))\nprint('1',arr1)\nprint('2',arr2)\nprint(np.where(arr1%2==0,arr1,100))\nprint('###################')\nprint(np.where(arr1&gt;arr2,arr1,arr2))\nlist1 = arr1.tolist()\nlist2 = arr2.tolist()\nprint([x if x&gt;y else y for x,y in zip(list1,list2)])\n\narr4 = np.random.uniform(10,50,(3,4))\n# 四舍五入之后大于20小于30的数字，修改为100\nprint(np.where((np.rint(arr4)&gt;20) &amp; (np.rint(arr4)&lt;30),100,arr4))\nprint(np.where(np.isnan(arr4),0,arr4))\n</code></pre>\n<h3>4.4常用的元素统计函数</h3>\n<pre><code>axis是指按照指定轴计算，0代表的是列，1代表的是行\nnp.mean() #求均值\nnp.sum() #求和\nnp.max() #最大值\nnp.min() #最小值\nnp.std() #标准差\nnp.var() #方差、\nnp.argmax() #最大值的下标索引\nnp.argmin() #最小值的下标索引\nnp.cumsum() #所有元素都是之前元素的累加，一维数组\nnp.cumprod() # 所有元素都是之前元素的累乘，一维数组\n\n\narr1 = np.arange(1,50).reshape(7,7)\nprint(arr1)\nprint('###################')\nprint(np.mean(arr1,axis=1)) #求均值\nprint(np.mean(arr1,axis=0)) #求均值\nprint(np.mean(arr1)) #求均值\nprint('###################')\nprint(np.sum(arr1)) #求和\nprint(np.max(arr1)) #最大值\nprint(np.min(arr1)) #最小值\nprint(np.std(arr1)) #标准差\nprint(np.var(arr1)) #方差、\nprint(np.argmax(arr1)) #最大值的下标索引\nprint(np.argmin(arr1)) #最小值的下标索引\nprint(np.cumsum(arr1)) #所有元素都是之前元素的累加，一维数组\nprint(np.cumprod(arr1)) # 所有元素都是之前元素的累乘，一维数组\n</code></pre>\n<h3>4.5 判断函数</h3>\n<pre><code>arr1 = np.random.randint(0,50,(4,4))\nprint(arr1)\nnp.all((arr1&gt;0)&amp;(arr1&lt;100))\nnp.all(arr1&gt;10,axis=1)\nnp.any(arr1&gt;10)\nnp.any(arr1&gt;10,axis=0)\n注意点：\n1、返回的是布尔值\n2、all 需要元素全部符合，any至少一位满足\n3、axis指定轴\n</code></pre>\n<h2>5 、数组的增加、插入、删除、合并</h2>\n<pre><code>append()\ninsert()\ndelete()\nconcatenate()\n</code></pre>\n<h3>5.1 append()</h3>\n<pre><code>import numpy as np\n\narr1 = np.arange(10,20)\narr2 = np.arange(20,30)\narr3 = np.arange(20).reshape(4,5)\narr4 = np.arange(20).reshape(4,5)\nprint(arr1)\nprint(arr2)\nprint(arr3)\nprint('##########append#####')\nprint('#######一维数组的append#####')\nprint(np.append(arr1,100))\nprint(np.append(arr3,100))\nprint(np.append(arr1,arr2))\nprint('%%%%%%%%%%%%%%%%%')\nprint(np.append(arr3,arr2))\nprint('****************')\nprint(np.append(arr3,arr4))\n\n展示：\n[10 11 12 13 14 15 16 17 18 19]\n[20 21 22 23 24 25 26 27 28 29]\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]]\n##########append#####\n#######一维数组的append#####\n[ 10  11  12  13  14  15  16  17  18  19 100]\n[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19 100]\n[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n%%%%%%%%%%%%%%%%%\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29]\n****************\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19  0  1  2  3\n  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n\n注意点：\n1、append是在数组的末尾做追加\n2、append返回的是一个新数组，对于原来数据不做操作\n3、多维数组追加一个数据，会返回一个一维数组\n4、一维数组追加一维数组，返回一个一维数组\n</code></pre>\n<h3>5.2 insert</h3>\n<pre><code class=\"language-python\">\n\nprint(np.insert(arr1,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">100</span>))\nprint(np.insert(arr3,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">100</span>))\nprint(np.insert(arr1,<span class=\"hljs-number\">0</span>,arr2))\nprint(np.insert(arr1,<span class=\"hljs-number\">0</span>,[<span class=\"hljs-number\">100</span>,<span class=\"hljs-number\">200</span>]))\n\nprint(np.insert(arr3,<span class=\"hljs-number\">0</span>,arr5,axis=<span class=\"hljs-number\">0</span>))\nprint(np.insert(arr3,<span class=\"hljs-number\">0</span>,arr6,axis=<span class=\"hljs-number\">1</span>))\n\n注意点：\n<span class=\"hljs-number\">1</span>、多维数组在指定位置上加入一个值，默认当做一位数组进行操作，返回的是一维数组\n<span class=\"hljs-number\">2</span>、insert（）参数整理\n\t第一个参数表示需要操作的数组\n\t第二个参数表示的是下标\n\t第三个是需要插入的值\n\t第四个表示的轴，<span class=\"hljs-number\">0</span>代表在每一列上增加数据，<span class=\"hljs-number\">1</span>代表在每一行上操作数据\n<span class=\"hljs-number\">3</span>、需要插入的数据能在原始数据上做广播\n</code></pre>\n<h3>5.3 delete</h3>\n<pre><code class=\"language-python\">print(np.delete(arr1,<span class=\"hljs-number\">0</span>))\nprint(arr1)\n\nprint(np.delete(arr3,<span class=\"hljs-number\">0</span>,axis=<span class=\"hljs-number\">0</span>))\nprint(<span class=\"hljs-string\">'############'</span>)\nprint(arr3)\n\n注意点：\n\n<span class=\"hljs-number\">1</span>、一维数组删除单个元素\n<span class=\"hljs-number\">2</span>、多维数组删除单个元素，会导致原始的结构发生变化，返回一维数组，所以一般情况下，配合axis指定行<span class=\"hljs-number\">1</span>(删除每一行的第几个元素，就是删除那一列)列<span class=\"hljs-number\">0</span>\n</code></pre>\n<p>###<strong>5.4 concatenate</strong></p>\n<pre><code>print(np.concatenate((arr1,arr2)))\nprint(11111111111111)\nprint(np.concatenate((arr3,arr4),axis=0))\nprint(2222222222222222222)\nprint(np.concatenate((arr3,arr4),axis=1))\n\n展示：\n[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n11111111111111\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]\n [ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]]\n2222222222222222222\n[[ 0  1  2  3  4  0  1  2  3  4]\n [ 5  6  7  8  9  5  6  7  8  9]\n [10 11 12 13 14 10 11 12 13 14]\n [15 16 17 18 19 15 16 17 18 19]]\n\n\n注意点：\n1、一维数组合并，默认按照行\n2、多维数组合并，axis=1 ，是指按照行合并，增加的是多列数据\n3、多维数组合并，axis=0 ，是指按照列合并，增加对的是多行数据\n</code></pre>\n<h2>6、数组的集合函数</h2>\n<pre><code>import numpy as np\n\n\n# s1 = {10,20,30,40}\n# s2 = {10,100,20,200}\n# #交集\n# print(s1&amp;s2)\n# print(s1|s2)\n\narr1 = np.arange(10,20)\narr2 = np.arange(0,5)\narr3 = np.array([100,10,10,10,20,30,40,50,1])\n\nprint('##########数组去重,排序##########')\nprint(np.unique(arr3))\nprint('#####数组的交集####')\nprint(np.intersect1d(arr1,arr3))\nprint('######数组的并集###')\nprint(np.union1d(arr1,arr2))\nprint('###差集:arr1有，但是arr2没有的##')\nprint(np.setdiff1d(arr1,arr2))\nprint(np.setdiff1d(arr2,arr1))\nprint('#######对称差集：二者差集的集合#####')\nprint(np.setxor1d(arr1,arr3))\nprint(np.setxor1d(arr3,arr1))\nprint('#########判断是否包含元素###')\nprint(np.in1d(arr1,arr2))\n\n展示：\n##########数组去重,排序##########\n[  1  10  20  30  40  50 100]\n#####数组的交集####\n[10]\n######数组的并集###\n[ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19]\n###差集:arr1有，但是arr2没有的##\n[10 11 12 13 14 15 16 17 18 19]\n[0 1 2 3 4]\n#######对称差集：二者差集的集合#####\n[  1  11  12  13  14  15  16  17  18  19  20  30  40  50 100]\n[  1  11  12  13  14  15  16  17  18  19  20  30  40  50 100]\n#########判断是否包含元素###\n[False False False False False False False False False False]\n\n</code></pre>\n<h2>7、数组排序</h2>\n<pre><code>def sort(a, axis=-1, kind='quicksort', order=None):\n    &quot;&quot;&quot;\n    Return a sorted copy of an array.\n    \n 注意点：\n 返回的是原始数据的复制版本排序之后的\n \n</code></pre>\n<pre><code>def sort(self, axis=-1, kind='quicksort', order=None): # real signature unknown; restored from __doc__\n    &quot;&quot;&quot;\n    a.sort(axis=-1, kind='quicksort', order=None)\n    \n        Sort an array, in-place.\n    \n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to sort. Default is -1, which means sort along the\n            last axis.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, optional\n            Sorting algorithm. Default is 'quicksort'.\n        order : str or list of str, optional\n            When `a` is an array with fields defined, this argument specifies\n            which fields to compare first, second, etc.  A single field can\n            be specified as a string, and not all fields need be specified,\n            but unspecified fields will still be used, in the order in which\n            they come up in the dtype, to break ties.\n    注意点：\n    在原始数据上做排序\n</code></pre>\n<h2>8、文件读写</h2>\n<p>###8.1 保存npy,npz格式，数组保存二进制文件</p>\n<pre><code>import numpy as np\n\narr1 = np.arange(20).reshape(4,5)\nnp.save('a1',arr1)\n\n#保存数组的二进制文件\n\n#读取数组的二进制文件\narr2 = np.load('a1.npy')\nprint(arr2)\n\n# np.savez()\n\n\nimport numpy as np\n\narr1 = np.arange(20).reshape(4,5)\narr2 = np.arange(20).reshape(4,5)\nnp.save('a1',arr1)\n\n#保存数组的二进制文件\n\n#读取数组的二进制文件\narr3 = np.load('a1.npy')\nprint(arr3)\n\nnp.savez('test',nd1=arr1,nd2=arr2)\narr4 = np.load('test.npz')\nprint('###########')\nprint(arr4['nd1'])\n展示：\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]]\n###########\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]]\n\n</code></pre>\n<h3>8.2 指定文件格式存储</h3>\n<pre><code>arr1 = np.array([\n['col1','col2','col3'],\n['java','python','go'],\n['mysql','redis','mongodb']\n])\nprint(arr1)\nnp.savetxt('aaa.csv',arr1,delimiter=',',fmt='%s')\n\n注意点：\ndef savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='',\n            footer='', comments='# ', encoding=None)\ndelimiter:分隔符，CSV文件需要指定','\nfmt指定的是数据的格式 '%s' '%d'  '%f'\n</code></pre>\n<pre><code>arr2 = np.genfromtxt('aaa.csv',delimiter=',',dtype=str,usecols=(1,))\nprint(arr2)\n\nfname, dtype=float, comments='#', delimiter=None,\nusecols=None\nusecols指定的是读取的列，下标从0 开始，如果元组，要使用','\n</code></pre>\n<p>##9、pandas---series</p>\n<h3>9.1 series对象</h3>\n<pre><code>import pandas as pd\n\n# series\nser_obj = pd.Series(data=list('asdf'))\nprint(ser_obj)\nprint(ser_obj.index)\nprint(ser_obj.values)\n\nprint('#########')\nprint(type(ser_obj.index))\nprint(type(ser_obj.values))\n\nser2 = pd.Series(np.random.randint(-5,50,10))\nprint(ser2)\n\n展示：\n0    a\n1    s\n2    d\n3    f\ndtype: object\nRangeIndex(start=0, stop=4, step=1)\n['a' 's' 'd' 'f']\n#########\n&lt;class 'pandas.core.indexes.range.RangeIndex'&gt;\n&lt;class 'numpy.ndarray'&gt;\n\n0    40\n1    23\n2    30\n3    -5\n4    10\n5    41\n6    41\n7    35\n8    10\n9    29\ndtype: int32\n\n注意点：\nseries = index对象+ ndarray对象\n</code></pre>\n<h3>9.2 series对象的参数</h3>\n<pre><code>ser3 = pd.Series(data=['python','java','c++','go'],index=list('abcd'),dtype=str,name='ser1')\nser4 = pd.Series(data=[1,2,3,4],index=list('abcd'),dtype=str,name='ser2')\nprint(ser3)\nprint(ser4)\n\n展示：\na    python\nb      java\nc       c++\nd        go\nName: ser1, dtype: object\na    1\nb    2\nc    3\nd    4\nName: ser2, dtype: object\n</code></pre>\n<h3>9.3 创建series对象的方法</h3>\n<pre><code>1、通过数组创建\nser2 = pd.Series(np.random.randint(-5,50,10))\n2、通过列表\nser3 = pd.Series(data=['python','java','c++','go'],index=list('abcd'),dtype=str,name='ser1')\n3、通过字典，字典的键作为index ，值作为values, int-----&gt;float------&gt;object\nprint('############################')\nser5 = pd.Series(data={'name':'wang','age':18})\nprint(ser5)\n</code></pre>\n<h2>10 、pandas-----DataFrame</h2>\n<h3>10.1 创建dataframe对象</h3>\n<pre><code>import numpy as np\nimport pandas as pd\n\n\ndf1 = pd.DataFrame(list('asdf'),columns=['M'])\nprint(df1)\nprint('%%%%%%%%%%%%%%%%%%')\ndf2 = pd.DataFrame(np.random.randint(10,100,(4,4)))\nprint(df2)\nprint('%%%%%%%%%%%%%%%%%')\ndict_new = {\n    'names':['大王','wang2','wang3'],\n    'age':18,\n    'country':'中国',\n}\ndf3 = pd.DataFrame(dict_new)\nprint(df3)\nprint(df3.dtypes)\nprint(df3.index)\nprint(df3.columns)\n\n展示：\n   M\n0  a\n1  s\n2  d\n3  f\n%%%%%%%%%%%%%%%%%%\n    x   y   z  z2\n0  36  67  88  20\n1  69  43  86  37\n2  68  79  22  11\n3  59  61  18  13\n%%%%%%%%%%%%%%%%%\n   age country  names\n0   18      中国     大王\n1   18      中国  wang2\n2   18      中国  wang3\ndtype: object\nRangeIndex(start=0, stop=3, step=1)\nIndex(['age', 'country', 'names', 'price'], dtype='object')\n\n注意点：\n1、dataframe对象创建可以通过一维数组和多维数组创建\n2、可以指定行，列索引，index指定的是行索引，columns指定的是列索引\n3、如果使用的是字典创建：\n\t（1）字典的键名作为列索引\n\t（2）单个的字符串或者数字，扩充到最大个数\n\t（3）如果是多个元素，保证元素的个数是相同的\n</code></pre>\n<h3>10.2 df对象的讲解</h3>\n<pre><code>print('^^^^^^^^^^^^^^^^')\nprint(df3['names'])\nprint(df3['names'].values)\nprint(type(df3['names']))\n展示：\nA       大王\nB    wang2\nC    wang3\nName: names, dtype: object\n['大王' 'wang2' 'wang3']\n&lt;class 'pandas.core.series.Series'&gt;\n\n注意点：\n1、dataframe对象是由很多个series对象组成的，只不过这些series对象共享一个行索引\n2、df[索引]获取了一列数据\n\n\n\nprint('查看当前df对象的详细信息')\nprint(df3.info())\n\n展示：\n查看当前df对象的详细信息\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 3 entries, A to C\nData columns (total 4 columns):\nage        3 non-null int64\ncountry    3 non-null object\nnames      3 non-null object\nprice      3 non-null float64\ndtypes: float64(1), int64(1), object(2)\nmemory usage: 120.0+ bytes\nNone\n\ndf3 = pd.DataFrame(dict_new,index=list('ABC'))\ndf3.index.name='index_name'\ndf3.columns.name = 'col_name'\n展示：\ncol_name    age country  names  price\nindex_name                           \nA            18      中国     大王   1.23\nB            18      中国  wang2   1.23\nC            18      中国  wang3   1.23\n\n\n</code></pre>\n<p>##11、pandas索引操作</p>\n<h3>11.1 series的索引操作</h3>\n<p>####11.1.1 series对象，index索引名不是数组的</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\n#series的索引操作\nser1 = pd.Series(range(10,15),index=list('ABCDE'))\nprint(ser1)\n\nprint('#######取单个数据的时候#######')\nprint(ser1['A'])\nprint(ser1[0])\nprint('########取连续数据#########')\nprint(ser1['A':'C'])\nprint(ser1[0:2])\nprint('如果取连续多个数据的时候，下标取值不包含结束位置，如果是索引切片，包含结束位置')\nprint('取不连续多个')\nprint(ser1[['A','C','E']])\nprint(ser1[[0,2,3]])\nprint('#########布尔索引（条件筛选）#########')\nprint(ser1[(ser1&gt;12) &amp; (ser1&lt;15)])\nprint(ser1[(ser1&lt;12)|(ser1&gt;15)])\nprint(ser1[ser1!=13])\nprint(ser1[~(ser1==13)])\n\n展示：\nA    10\nB    11\nC    12\nD    13\nE    14\ndtype: int64\n#######取单个数据的时候#######\n10\n10\n########取连续数据#########\nA    10\nB    11\nC    12\ndtype: int64\nA    10\nB    11\ndtype: int64\n如果取连续多个数据的时候，下标取值不包含结束位置，如果是索引切片，包含结束位置\n取不连续多个\nA    10\nC    12\nE    14\ndtype: int64\nA    10\nC    12\nD    13\ndtype: int64\n#########布尔索引（条件筛选）#########\nD    13\nE    14\ndtype: int64\nA    10\nB    11\ndtype: int64\nA    10\nB    11\nC    12\nE    14\ndtype: int64\nA    10\nB    11\nC    12\nE    14\ndtype: int64\n注意点：\n1、索引名是字符串类型的时候，取单个值既可以通过索引名，也可以通过下标，但是如果索引名也是数组的时候，默认按照索引名进行取值\n2、取连续多个数据，如果使用下标，不包含结束位，使用索引包含结束位\n3、取多个数据返回的是一个series对象\n</code></pre>\n<h4>11.1.2 series对象，但是索引名是数字</h4>\n<pre><code>print('**********************')\nser2 = pd.Series(range(10,15),index=range(1,6))\nprint(ser2)\nprint(ser2[1])\n\n展示：\n**********************\n1    10\n2    11\n3    12\n4    13\n5    14\ndtype: int64\n10\n\n注意点：\n但是如果索引名也是数组的时候，默认按照索引名进行取值\n</code></pre>\n<h3>11.2 dataframe的索引操作</h3>\n<h4>11.2.1 列索引取值</h4>\n<h5>11.2.1.1 取出一列数据</h5>\n<pre><code>df1 = pd.DataFrame(np.random.randint(10,50,(3,4)),index=list('ABC'),columns=list('abcd'))\nprint(df1)\nprint('######取出一列数据#######')\nprint(df1['a'])\nprint(df1['a'].values)\nprint('取出的数据是一个series对象')\n展示：\n    a   b   c   d\nA  20  34  40  26\nB  18  13  39  17\nC  15  33  40  36\n######取出一列数据#######\nA    20\nB    18\nC    15\nName: a, dtype: int32\n[20 18 15]\n取出的数据是一个series对象\n</code></pre>\n<h5>11.2.1.2 取出一列数据的某一行数据（取单个数据）</h5>\n<pre><code>print('取出一列数据的某一行数据（取单个数据）')\nprint(df1['a']['B'])\nprint(df1['a'][1])\n展示：\n取出一列数据的某一行数据（取单个数据）\n33\n33\n\n</code></pre>\n<h5>11.2.1.3 取不连续多列</h5>\n<pre><code>print('########不连续多列######')\nprint(df1[['a','c']])\n展示：\n########不连续多列######\n    a   c\nA  21  20\nB  19  13\nC  41  19\n\n</code></pre>\n<p>#####11.2.1.4 取连续多列</p>\n<p>注意点：默认不支持取连续多列数据，通过高级索引实现</p>\n<h4>11.2.2 行索引取值</h4>\n<h5>11.2.2.1 取单行</h5>\n<pre><code>print('11.2.2.1 取单行')\nprint(df1['A':'A'])\n展示\n11.2.2.1 取单行\n    a   b   c   d\nA  32  16  29  17\n</code></pre>\n<h5>11.2.2.2 取连续多行</h5>\n<pre><code>print('11.2.2.2 取连续多行')\nprint(df1['A':'C'])\n展示：\n11.2.2.2 取连续多行\n    a   b   c   d\nA  32  16  29  17\nB  10  29  41  16\nC  34  49  35  22\n</code></pre>\n<h5>11.2.2.3 取不连续多行</h5>\n<p>默认不支持取不连续多行</p>\n<h3>11.3 高级索引</h3>\n<h4>11.3.1 标签索引  loc</h4>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\"># series取值</span>\n<span class=\"hljs-comment\"># print(ser1['A'])</span>\n<span class=\"hljs-comment\"># print(ser1['A':'C'])</span>\n<span class=\"hljs-comment\"># print(ser1[['A','C','E']])</span>\nprint(ser1.loc[<span class=\"hljs-string\">'B'</span>])\nprint(ser1.loc[<span class=\"hljs-string\">'B'</span>:<span class=\"hljs-string\">'D'</span>])\nprint(ser1.loc[[<span class=\"hljs-string\">'B'</span>,<span class=\"hljs-string\">'D'</span>]])\n展示：\n    a   b   c   d\nA  <span class=\"hljs-number\">28</span>  <span class=\"hljs-number\">43</span>  <span class=\"hljs-number\">22</span>  <span class=\"hljs-number\">21</span>\nB  <span class=\"hljs-number\">34</span>  <span class=\"hljs-number\">35</span>  <span class=\"hljs-number\">27</span>  <span class=\"hljs-number\">39</span>\nC  <span class=\"hljs-number\">11</span>  <span class=\"hljs-number\">41</span>  <span class=\"hljs-number\">15</span>  <span class=\"hljs-number\">44</span>\nA    <span class=\"hljs-number\">10</span>\nB    <span class=\"hljs-number\">11</span>\nC    <span class=\"hljs-number\">12</span>\nD    <span class=\"hljs-number\">13</span>\nE    <span class=\"hljs-number\">14</span>\ndtype: int64\n<span class=\"hljs-comment\">#标签索引  loc##</span>\n<span class=\"hljs-number\">11</span>\nB    <span class=\"hljs-number\">11</span>\nC    <span class=\"hljs-number\">12</span>\nD    <span class=\"hljs-number\">13</span>\ndtype: int64\nB    <span class=\"hljs-number\">11</span>\nD    <span class=\"hljs-number\">13</span>\ndtype: int64\n\n</code></pre>\n<pre><code>print('取单行')\nprint(df1['A':'A'])\nprint(df1.loc['A'])\n展示：\n    a   b   c   d\nA  20  11  17  25\na    20\nb    11\nc    17\nd    25\nName: A, dtype: int32\n注意点：\n原始的取值方式，取出的是一个df对象\n标签索引取单行数据，取出的是一个series对象\n\n</code></pre>\n<pre><code>print('取单列')\nprint(df1['a'])\nprint(df1.loc[:,'a'])\n展示：\n取单列\nA    21\nB    22\nC    26\nName: a, dtype: int32\nA    21\nB    22\nC    26\nName: a, dtype: int32\n</code></pre>\n<pre><code>print('取连续多行')\nprint(df1['A':'C'])\nprint(df1.loc['A':'C'])\n\n\n展示\n取连续多行\n    a   b   c   d\nA  25  37  47  33\nB  36  20  26  17\nC  46  44  19  31\n    a   b   c   d\nA  25  37  47  33\nB  36  20  26  17\nC  46  44  19  31\n</code></pre>\n<pre><code>print('####取连续多列###')\nprint(df1.loc[:,'a':'c'])\n展示：\n####取连续多列###\n    a   b   c\nA  34  44  40\nB  37  24  38\nC  44  21  11\n\n</code></pre>\n<pre><code>print('#####取连续多行多列####')\nprint(df1.loc['B':'C','c':'d'])\n展示：\n#####取连续多行多列####\n    c   d\nB  36  12\nC  42  38\n</code></pre>\n<pre><code>print('#取不连续多行#')\nprint(df1.loc[['A','C']])\n展示：\n取不连续多行#\n    a   b   c   d\nA  43  19  16  16\nC  33  10  41  48\n</code></pre>\n<pre><code>print('###取不连续多列###')\nprint(df1.loc[:,['a','c','d']])\n展示：\n###取不连续多列###\n    a   c   d\nA  24  21  36\nB  49  33  29\nC  40  41  45\n\n</code></pre>\n<pre><code>print('###取不连续的多行多列##')\nprint(df1.loc[['A','C'],['a','d','c']])\n展示\n###取不连续的多行多列##\n    a   d   c\nA  45  32  43\nC  34  46  15\n</code></pre>\n<h4>11.3.2 位置索引 iloc（根据下标取值，不包含结束位）</h4>\n<pre><code>print(df1.iloc[0,0])\nprint(df1.iloc[0:2])\nprint(df1.iloc[:,0:3])\nprint(df1.iloc[[0,2],[0,3]])\n展示：\n    a   b   c   d\nA  45  44  36  10\nB  14  42  46  11\nC  46  18  20  17\nA    10\nB    11\nC    12\nD    13\nE    14\ndtype: int64\n#标签索引  loc##\n45\n    a   b   c   d\nA  45  44  36  10\nB  14  42  46  11\n    a   b   c\nA  45  44  36\nB  14  42  46\nC  46  18  20\n    a   d\nA  45  10\nC  46  17\n</code></pre>\n<h4>11.3.3 混合索引 ix</h4>\n<pre><code>print(df1.ix['A':'B',0:3])\n展示：\n    a   b   c\nA  29  34  20\nB  17  42  34\n\n</code></pre>\n<h4>11.3.4 根据索引增加行/列数据</h4>\n<pre><code>df1 = pd.DataFrame(np.random.randint(10,20,(3,4)),index=list('ABC'),columns=list('accd'))\nprint(df1)\nprint('增加一列数据')\ndf1['e'] = df1['d']*10\nprint(df1)\nprint('增加一行数据')\ndf1.loc['D'] = [1,2,3,4,5]\nprint(df1)\nprint('将各列数据相加之和作为下一列')\ndf1['f'] = df1['a']+df1['b']+df1['c']+df1['d']+df1['e']\nprint(df1)\n\n展示：\n    a   c   c   d\nA  16  14  17  16\nB  17  18  11  10\nC  13  18  19  10\n增加一列数据\n    a   c   c   d    e\nA  16  14  17  16  160\nB  17  18  11  10  100\nC  13  18  19  10  100\n增加一行数据\n    a   c   c   d    e\nA  16  14  17  16  160\nB  17  18  11  10  100\nC  13  18  19  10  100\nD   1   2   3   4    5\n\n将各列数据相加之和作为下一列\n    a   b   c   d    e    f\nA  12  16  11  15  150  204\nB  10  18  16  11  110  165\nC  11  13  13  15  150  202\nD   1   2   3   4    5   15\n\n</code></pre>\n<h4>11.3.5 根据索引删除数据</h4>\n<pre><code>def drop(self, labels=None, axis=0, index=None, columns=None, level=None,\n         inplace=False, errors='raise'):\n    &quot;&quot;&quot;\n    Return new object with labels in requested axis removed.\n\n    Parameters\n    ----------\n    labels : single label or list-like\n        Index or column labels to drop.\n    axis : int or axis name\n        Whether to drop labels from the index (0 / 'index') or\n        columns (1 / 'columns').\n    index, columns : single label or list-like\n        Alternative to specifying `axis` (``labels, axis=1`` is\n        equivalent to ``columns=labels``).\n\n        .. versionadded:: 0.21.0\n    level : int or level name, default None\n        For MultiIndex\n    inplace : bool, default False\n        If True, do operation inplace and return None.\n    errors : {'ignore', 'raise'}, default 'raise'\n        If 'ignore', suppress error and existing labels are dropped.\n</code></pre>\n<pre><code>df1 = pd.DataFrame(np.random.randint(10,20,(3,4)),index=list('ABC'),columns=list('abcd'))\n# print(df1)\n# print('增加一列数据')\n# df1['e'] = df1['d']*10\n# print(df1)\n# print('增加一行数据')\n# df1.loc['D'] = [1,2,3,4,5]\n# print(df1)\n# print('将各列数据相加之和作为下一列')\n# df1['f'] = df1['a']+df1['b']+df1['c']+df1['d']+df1['e']\n# print(df1)\nprint('原始数据')\nprint(df1)\nprint('删除多列数据')\ndf2 = df1.drop(['c','d'],axis=1)\nprint(df1)\nprint(df2)\n\n注意点：\n删除指定的索引，必须配合axis使用，axis=1 表示列，axis=0表示的是行\ndrop()这个方法，返回删除之后的dataframe，原始数据不受影响\n</code></pre>\n<pre><code># del  df2\n# print(df2)\ndel df1['d']\ndel df1.loc['A']\nprint(df1)\n\n展示：\n    a   b\nA  11  13\nB  14  17\nC  14  18\nTraceback (most recent call last):\n  File &quot;C:/Users/Administrator/Desktop/python_new/数组/pandas索引操作.py&quot;, line 122, in &lt;module&gt;\n    del df1.loc['A']\nAttributeError: __delitem__\n\ndel 这个方法，只能删除列，不能删除行\n而且实在原始数据上做的操作\n\n</code></pre>\n<h4>11.3.6 索引对象的转换</h4>\n<pre><code>print(df1)\nindex_data = df1.index\ncolumns_data = df1.columns\nindex_data_list = list(index_data)\nprint('tolist',index_data_list)\nprint(np.array(index_data))\n\nprint('###################')\nprint(ser1)\nprint(ser1.index)\n\n索引对象，支持转成列表，数组，series也能转\n</code></pre>\n<h2>12、pandas的对齐操作</h2>\n<p>四种主要的运算：加add（）减sub()乘mul()除div()</p>\n<h3>12.1 series</h3>\n<h5>12.1.1 加法</h5>\n<pre><code>import numpy as np\nimport pandas as pd\n\nprint('######series对齐运算####')\nser1 = pd.Series(data=range(10,15),index=list('abcde'))\nser2 = pd.Series(data=range(20,25),index=list('cdefg'))\nprint('###################ser1')\nprint(ser1)\nprint('###################ser2')\nprint(ser2)\nprint('###################add()')\nser_obj3 = ser1.add(ser2)\nprint(ser_obj3)\n\n展示：\n######series对齐运算####\n###################ser1\na    10\nb    11\nc    12\nd    13\ne    14\ndtype: int64\n###################ser2\nc    20\nd    21\ne    22\nf    23\ng    24\ndtype: int64\n###################add()\na     NaN\nb     NaN\nc    32.0\nd    34.0\ne    36.0\nf     NaN\ng     NaN\ndtype: float64\n注意点：\n1、series对象在进行相加操作的时候，它会将相同索引的对应的值进行相加操作\n2、如果索引不同的话，就会使用NaN值进行填充操作 \n3、fill_value这个参数制定一个填充值，如果没有相同的索引，可以使用填充值进行操作\n\n\nser_obj4 = ser1.add(ser2,fill_value=0)\nprint(ser_obj4)\n展示：\na    10.0\nb    11.0\nc    32.0\nd    34.0\ne    36.0\nf    23.0\ng    24.0\ndtype: float64\n</code></pre>\n<h3>12.2 dataframe</h3>\n<h4>12.2.1 加法</h4>\n<pre><code>print('dataframe对齐运算')\ndf1 = pd.DataFrame(np.arange(11,20).reshape(3,3),columns=list('ABC'))\ndf2 = pd.DataFrame(np.arange(20,40).reshape(4,5),columns=list('ABCDE'))\n\nprint(df1)\nprint(df2)\n\ndf3 = df1.add(df2,fill_value=0)\nprint(df3)\n注意点：\n和serie对象的相加是一样，不需要是相同的数据\n\n\n\n</code></pre>\n<h2>13、数据清洗</h2>\n<p>###13.1 判断是否存在空值</p>\n<pre><code>df1 = pd.DataFrame([np.random.randint(10,50,4),[1.1,2.2,3.3,]])\nprint(df1)\n\nprint('####判断是否存在缺失值##')\nret = df1.isnull()\nprint(ret)\nret2 = df1.notnull()\nprint(ret2)\n展示：\n####判断是否存在缺失值##\n       0      1      2      3\n0  False  False  False  False\n1  False  False  False   True\n      0     1     2      3\n0  True  True  True   True\n1  True  True  True  False\n\n</code></pre>\n<h3>13.2 删除存在空值的行/列</h3>\n<pre><code>ret3 = df1.dropna(axis=1,inplace=True)\nprint(ret3)\nprint(df1)\n\n1、dropna这个方法，默认删除一行\n2、axis=1 列\n3、inplace的参数默认值是false，代表不在原始数据上做操作，返回删除空值之后的数据，如果设置为True则，在原始数据做操作\n</code></pre>\n<h3>13.3 填充缺失值</h3>\n<pre><code># ret3 = df1.fillna(0)\n# print(ret3)\n\nret3 = df1.fillna({0:0,3:100})\nprint(ret3)\n注意点：\n1、如果在fillna这个函数中填值，代表所有的nan数据直接换成指定的值\n2、如果参数是字典，那么表示对于不同的列有不同的值，字典的键名是列名，值为填充的值\n</code></pre>\n<h3>13.4 处理重复数据</h3>\n<pre><code>duplicated()\n对于指定列的每一个元素进行判断，如果已经出现该元素，标记为True,返回的是一个series对象\n\n\nret2 = df1.drop_duplicates('data1')\nprint(ret2)\n\n展示：\n  data1  data2\n0     a      1\n1     a      2\n2     b      3\n3     b      4\n4     c      1\n5     c      2\n6     d      3\n7     d      5\n  data1  data2\n0     a      1\n2     b      3\n4     c      1\n6     d      3\n注意点：\n1、指定inplace可以指定是否在原始数据上做操作\n</code></pre>\n<h3>13.5 替换</h3>\n<pre><code># ret1 = df1.replace('b',1,)\n将数据中的'b'全部替换成1\n# ret1 = df1.replace(['b','d'],1,)\n将全部的b和d，替换成1\nret1 = df1.replace({'b':1,'d':2})\n将b替换成1，'d'替换成2\nret2 = ret1.replace({'data1':1},'m')\n将data1这一列的1，替换成'm'\nprint(ret1)\nprint(ret2)\n\n展示：\n  data1  data2\n0     a      1\n1     a      2\n2     b      3\n3     b      4\n4     c      1\n5     c      2\n6     d      3\n7     d      5\n  data1  data2\n0     a      1\n1     a      2\n2     1      3\n3     1      4\n4     c      1\n5     c      2\n6     2      3\n7     2      5\n  data1  data2\n0     a      1\n1     a      2\n2     m      3\n3     m      4\n4     c      1\n5     c      2\n6     2      3\n7     2      5\n\n</code></pre>\n<h2>14、pandas中的函数应用</h2>\n<h3>14.1 求绝对值</h3>\n<pre><code>import numpy as np\nimport pandas as pd\n\nser1 = pd.Series(np.random.randint(-10,10,10))\ndf1 = pd.DataFrame(np.random.randint(-10,10,(4,5)))\nprint(ser1)\nprint(np.abs(ser1))\nprint(df1)\n展示：\n0   -8\n1   -6\n2   -9\n3    0\n4   -9\n5   -2\n6    2\n7    5\n8    0\n9   -5\ndtype: int32\n0    8\n1    6\n2    9\n3    0\n4    9\n5    2\n6    2\n7    5\n8    0\n9    5\ndtype: int32\n   0   1  2  3  4\n0  4   1  8  1  7\n1 -5   5 -8 -2  6\n2  0  -9 -5 -7 -1\n3  7 -10  3  9 -3\n</code></pre>\n<h3>14.2 求和</h3>\n<pre><code>print(np.sum(ser1))\nprint(np.sum(df1,axis=1))\n展示：\n-10\n0   -21\n1    13\n2     6\n3     0\ndtype: int64\n注意点：\n1、axis=1 行，axis=0 列\n</code></pre>\n<h3>14.3 apply（）将自定义函数作用到dataframe的行或者列或者series的列上</h3>\n<pre><code>def func(x):\n    # print(x)\n    num = np.max(x)-np.min(x)\n    # print(num)\n    return num\nprint(df1)\nprint('#########################')\nprint(df1.apply(func,axis=1))\nprint(df1.apply(lambda x:np.max(x)-np.min(x),axis=1))\nprint(df1.apply(lambda x:x**2,axis=1))\nprint(df1.apply(lambda x:x*x,axis=1))\n\n注意点：\n1、apply这个函数是将自定义的函数应用于每一行或者列上的\n2、如果自定义的函数是一个计算型的函数，将应用在每一个元素上面\n3、如果自定义的函数是一个统计型的函数，将应用于每一列或者每一行\n4、axis=0 代表的是列，axis=1代表的是行\n</code></pre>\n<h3>14.4applymap()将自定义的函数应用于每一个元素</h3>\n<pre><code>def func1(x):\n    # print(x)\n    return abs(x)\n\nprint(df1.applymap(func1))\nprint(df1.applymap(lambda x:abs(x)))\n</code></pre>\n<h2>15、排序</h2>\n<h3>15.1 按照索引排序</h3>\n<pre><code>print('######按照索引排序############')\nprint(ser1.sort_index()) #默认按照升序进行排序\nprint(ser1.sort_index(ascending=False)) #按照降序\n\nprint(df1)\n</code></pre>\n<h3>15.2 按照值进行排序</h3>\n<pre><code>print('#######按照值进行排序##########')\nprint(ser1.sort_values()) #升序\nprint(ser1.sort_values(ascending=False))  #降序\n\nprint(df1.sort_values(by='a'))\n\n注意点：\n1、series对象的结构比较简单\n2、dataframe对象比较复杂，需要指定按照那一列来进行排序\n</code></pre>\n<h2>16、数据重构</h2>\n<h3>16.1 常见的索引类型</h3>\n<p>MultiIndex</p>\n<h3>16.2 层级索引</h3>\n<pre><code>MultiIndex(levels=[['a', 'b', 'c', 'd', 'e'], [1, 2, 3, 4, 5, 6, 7, 8, 9, 11]],\n           labels=[[0, 0, 1, 1, 2, 2, 3, 3, 4, 4], [0, 3, 4, 5, 1, 2, 6, 7, 8, 9]])\n           \n           levels用来展示两个层级中，有那些索引\n           labels说明每一个位置是那些标签\n</code></pre>\n<h3>16.3 按照层级取值</h3>\n<pre><code>print('########按照外层索引取值######')\nprint(ser1['a'])\nprint(type(ser1['a']))\n\nprint('####按照内层索引取值#####')\nprint(ser1[:,1])\n\nprint('#######指定外层与内层###')\nprint(ser1['c',2])\n</code></pre>\n<h3>16.4 交换分层</h3>\n<pre><code>print(ser1.swaplevel())\n展示：\nMultiIndex(levels=[['a', 'b', 'c', 'd', 'e'], [1, 2]],\n           labels=[[0, 0, 1, 1, 2, 2, 3, 3, 4, 4], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]])\na  1    9\n   2    1\nb  1   -9\n   2   -9\nc  1    2\n   2   -5\nd  1   -1\n   2    1\ne  1   -8\n   2    4\ndtype: int32\n1  a    9\n2  a    1\n1  b   -9\n2  b   -9\n1  c    2\n2  c   -5\n1  d   -1\n2  d    1\n1  e   -8\n2  e    4\ndtype: int32\n</code></pre>\n<h3>16.5 数据重构</h3>\n<pre><code># print(ser1.swaplevel())\nprint(ser1.unstack())\n将有层级索引的series对象转化成dataframe对象，外层索引作为行索引，内层索引作为列索引\n\nprint(df1.stack())\n将dataframe对象转成serie对象\n\nprint(df1)\nprint('#################')\n# print(df1.T)\n行列转置\nprint(df1.to_dict())\nprint(df1.to_csv())\n</code></pre>\n<h2>17、pandas的统计与汇总</h2>\n<h3>17.1 求和</h3>\n<pre><code>print(df1.sum())\nprint(df1.sum(axis=1))\n默认按照列求和，axis=0 是列，axis=1行\n</code></pre>\n<h3>17.2 求均值</h3>\n<pre><code>print(df1.mean())\nprint(df1.mean(skipna=False))\nprint(df2.mean())\nprint(df2.mean(skipna=True))\nprint(df2.mean(skipna=False))\n\n</code></pre>\n<h3>17.3 详细描述</h3>\n<pre><code>print(df1.describe())\n\n</code></pre>\n<h3>17.4 中位数</h3>\n<pre><code>print(df1.median())\n</code></pre>\n<h3>17.5 方差</h3>\n<p>统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的<a href=\"https://baike.baidu.com/item/%E5%B9%B3%E5%9D%87%E6%95%B0/11031224\">平均数</a></p>\n<pre><code>print(df1.var())\n\n</code></pre>\n<h3>17.6 标准差</h3>\n<pre><code>print(df1.std())\n</code></pre>\n<h2>18、数据连接</h2>\n<p>定义：根据单个或者多个键将不同的dataframe对象的行连接起来</p>\n<h3>18.1 merge</h3>\n<pre><code>pd.merge()\ndef merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n          left_index=False, right_index=False, sort=False,\n          suffixes=('_x', '_y'), copy=True, indicator=False,\n          validate=None):\n</code></pre>\n<pre><code>print('#########连接###########')\nprint(pd.merge(df1,df2,on='key'))\nprint(pd.merge(df1,df2))\n\n\nprint(pd.merge(df1,df2,on='key'))\nprint(pd.merge(df1,df2,on='data1'))\n注意点：\n1、两张表进行连接的过程中，如果没有指定，默认使用的是dataframe对象的同名的列作为外键进行关联\n2、如果同名的列很多，就可以通过on来指定外键\n\n</code></pre>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf1 = pd.DataFrame(\n    {'key1':list('abcdefgh'),'data1':np.random.randint(-5,10,8)}\n)\n\ndf2 = pd.DataFrame(\n    {'key2':list('abcde'),'data2':np.random.randint(-5,10,5)}\n)\n\nprint(df1)\nprint(df2)\n\nprint('#########连接###########')\n# print(pd.merge(df1,df2,on='key'))\n# print(pd.merge(df1,df2,on='data1'))\nprint(pd.merge(df1,df2,left_on='key1',right_on='key2'))\n\n展示：\n   data1 key1\n0     -1    a\n1     -1    b\n2      0    c\n3      3    d\n4      9    e\n5      0    f\n6      5    g\n7     -1    h\n   data2 key2\n0      8    a\n1      8    b\n2      4    c\n3     -5    d\n4      6    e\n#########连接###########\n   data1 key1  data2 key2\n0     -1    a      8    a\n1     -1    b      8    b\n2      0    c      4    c\n3      3    d     -5    d\n4      9    e      6    e\n注意点：\n1、当两张表没有同名的列的时候，可以通过left_on，right_on来指定连接的条件（列）\n</code></pre>\n<h3>18.2可以通过how这个参数指定连接方式， 默认的多表关联方式是内连接（多表的交集）</h3>\n<pre><code>print(pd.merge(df1,df2,left_on='key1',right_on='key2'))\nprint('#######outer####')\nprint(pd.merge(df1,df2,how='outer',left_on='key1',right_on='key2'))\nprint('######left####')\nprint(pd.merge(df1,df2,how='left',left_on='key1',right_on='key2'))\nprint('##right###')\nprint(pd.merge(df1,df2,how='right',left_on='key1',right_on='key2'))\n\n注意点：\n1、how=inner，默认的，内连接，多表的交集\n2、how=outer ，这是外连接，多表的并集\n3、how=left，指定为左连接，展示左表的完整数据，不管右表有没有匹配\n4、how=right，指定为右连接，不管左表有没有匹配，完整的展示右表的数据\n\n</code></pre>\n<p>###18.3dataframe对象的列全部都是同名的时候</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf1 = pd.DataFrame(\n    {'key1':list('abcdefgh'),'data1':np.random.randint(-5,10,8)}\n)\n\ndf2 = pd.DataFrame(\n    {'key2':list('abcde'),'data1':np.random.randint(-5,10,5)}\n)\n\nprint(df1)\nprint(df2)\n\nprint('#########连接###########')\n# print(pd.merge(df1,df2,on='key'))\n# print(pd.merge(df1,df2,on='data1'))\nprint('#####inner####')\nprint(pd.merge(df1,df2,left_on='key1',right_on='key2'))\n展示：\n   data1 key1\n0      4    a\n1      5    b\n2      1    c\n3      7    d\n4      0    e\n5      0    f\n6      5    g\n7      2    h\n   data1 key2\n0     -1    a\n1      3    b\n2      4    c\n3      7    d\n4     -3    e\n#########连接###########\n#####inner####\n   data1_x key1  data1_y key2\n0        4    a       -1    a\n1        5    b        3    b\n2        1    c        4    c\n3        7    d        7    d\n4        0    e       -3    e\n\n代码：\nprint('#####inner####')\nprint(pd.merge(df1,df2,left_on='key1',right_on='key2',suffixes=['_左表','_右表']))\n展示：\n#####inner####\n   data1_左表 key1  data1_右表 key2\n0         9    a         6    a\n1         5    b        -4    b\n2         5    c         3    c\n3         7    d         7    d\n4        -5    e         8    e\n\n\nprint('通过行索引来指定')\nprint(pd.merge(df1,df2,left_on='data1',right_index=True ))\n展示：\n   data1 key1\n0     -3    a\n1      0    b\n2      1    c\n3     -2    d\n4     -1    e\n5      5    f\n6      3    g\n7      4    h\n   data2 key1\n0      5    a\n1      3    b\n2      1    c\n3     -4    d\n4     -3    e\n通过行索引来指定\n   data1 key1_x  data2 key1_y\n1      0      b      5      a\n2      1      c      3      b\n6      3      g     -4      d\n7      4      h     -3      e\n\n\n\nprint('通过行索引来指定')\nprint(pd.merge(df1,df2,left_index=True,right_index=True ))\n\n展示：\n   data1 key1\n0     -1    a\n1      8    b\n2     -5    c\n3      0    d\n4     -3    e\n5     -5    f\n6      9    g\n7      7    h\n   data2 key1\n0     -1    a\n1      6    b\n2      8    c\n3      6    d\n4      7    e\n通过行索引来指定\n   data1 key1_x  data2 key1_y\n0     -1      a     -1      a\n1      8      b      6      b\n2     -5      c      8      c\n3      0      d      6      d\n4     -3      e      7      e\n\n\nprint('通过行索引来指定')\n# print(pd.merge(df1,df2,left_on='data1',right_index=True ))\nprint(pd.merge(df1,df2,how='left',left_index=True,right_index=True ))\nprint('################')\nprint(pd.merge(df1,df2,how='right',left_index=True,right_index=True ))\n展示：\n通过行索引来指定\n   data1 key1_x  data2 key1_y\n0      9      a    2.0      a\n1      9      b   -2.0      b\n2     -1      c    8.0      c\n3      0      d   -1.0      d\n4      4      e    0.0      e\n5     -2      f    NaN    NaN\n6      1      g    NaN    NaN\n7      2      h    NaN    NaN\n################\n   data1 key1_x  data2 key1_y\n0      9      a      2      a\n1      9      b     -2      b\n2     -1      c      8      c\n3      0      d     -1      d\n4      4      e      0      e\n\n注意点：\n1、如果两个对象出现同名的列的时候，默认是将_x和_y加在左右表的同名列名上\n2、suffixes=['_左表','_右表']，用来指定同名列的后缀，参数列表的顺序就是左右表的顺序\n3、left_on用来指定左表的列为外键，right是指将右表的行索引进行外键\n4、left_index,right_index,默认的情况下，将两张表按照行索引相同的情况，合成一条数据\n5、按照行进行连接，也可以指定连接方式\n\n</code></pre>\n<h2>19、数据的合并</h2>\n<h3>19.1 numpy的合并 np.concatenate()</h3>\n<pre><code>def concatenate(a_tuple, axis=0, out=None): # real signature unknown; restored from __doc__\n    &quot;&quot;&quot;\n    concatenate((a1, a2, ...), axis=0, out=None)\n    \n        Join a sequence of arrays along an existing axis.\n    \n        Parameters\n        ----------\n        a1, a2, ... : sequence of array_like\n            The arrays must have the same shape, except in the dimension\n            corresponding to `axis` (the first, by default).\n        axis : int, optional\n            The axis along which the arrays will be joined.  Default is 0.\n        out : ndarray, optional\n            If provided, the destination to place the result. The shape must be\n            correct, matching that of what concatenate would have returned if no\n            out argument were specified.\n            \n   \n</code></pre>\n<pre><code>arr1 = np.random.randint(10,20,(3,4))\narr2 = np.random.randint(10,20,(3,4))\n\nprint(np.concatenate((arr1,arr2),axis=1))\nprint('####################')\nprint(np.concatenate((arr1,arr2)))\n展示：\n\n[[19 17 18 13 11 18 19 15]\n [18 17 11 14 12 11 12 15]\n [12 13 17 19 16 17 11 17]]\n####################\n[[19 17 18 13]\n [18 17 11 14]\n [12 13 17 19]\n [11 18 19 15]\n [12 11 12 15]\n [16 17 11 17]]\n\n注意点：\n1、参与合并的数组，维度的大小是一致的\n2、axis默认为按照列进行合并，axis=1，按照行进行排序\n\n\n</code></pre>\n<h3>19.2 pandas的合并 pd.concat()</h3>\n<h4>19.2.1 concat（）</h4>\n<pre><code>def concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n           keys=None, levels=None, names=None, verify_integrity=False,\n           copy=True):\n           \n           \n           Parameters\n    ----------\n    objs : a sequence or mapping of Series, DataFrame, or Panel objects\n        If a dict is passed, the sorted keys will be used as the `keys`\n        argument, unless it is passed, in which case the values will be\n        selected (see below). Any None objects will be dropped silently unless\n        they are all None in which case a ValueError will be raised\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatenate along\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis(es)\n    join_axes : list of Index objects\n        Specific indexes to use for the other n - 1 axes instead of performing\n        inner/outer set logic\n    ignore_index : boolean, default False\n        If True, do not use the index values along the concatenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatenating objects where the concatenation axis does not have\n        meaningful indexing information. Note the index values on the other\n        axes are still respected in the join.\n    keys : sequence, default None\n        If multiple levels passed, should contain tuples. Construct\n        hierarchical index using the passed keys as the outermost level\n    levels : list of sequences, default None\n        Specific levels (unique values) to use for constructing a\n        MultiIndex. Otherwise they will be inferred from the keys\n    names : list, default None\n        Names for the levels in the resulting hierarchical index\n    verify_integrity : boolean, default False\n        Check whether the new concatenated axis contains duplicates. This can\n        be very expensive relative to the actual data concatenation\n    copy : boolean, default True\n        If False, do not copy data unnecessarily\n\n</code></pre>\n<h4>19.2.2 series合并</h4>\n<pre><code>ser1 = pd.Series(np.random.randint(10,20,3))\nser2 = pd.Series(np.random.randint(10,20,5))\nser3 = pd.Series(np.random.randint(10,20,7))\nprint(ser1)\nprint(ser2)\nprint(ser3)\n\nprint('#######series合并########')\nprint(pd.concat([ser1,ser2,ser3]))\n\n展示：\n0    18\n1    19\n2    12\ndtype: int32\n0    11\n1    14\n2    13\n3    13\n4    17\ndtype: int32\n0    18\n1    13\n2    19\n3    12\n4    13\n5    12\n6    11\ndtype: int32\n#######series合并########\n0    18\n1    19\n2    12\n0    11\n1    14\n2    13\n3    13\n4    17\n0    18\n1    13\n2    19\n3    12\n4    13\n5    12\n6    11\ndtype: int32\n\n\n\n\n\n\n\n\n\n\n\n\nprint('#######series合并########')\nprint(pd.concat([ser1,ser2,ser3],axis=1))\n展示：\n      0     1   2\n0  11.0  10.0  18\n1  14.0  13.0  18\n2  15.0  10.0  15\n3   NaN  16.0  19\n4   NaN  10.0  16\n5   NaN   NaN  18\n6   NaN   NaN  18\n\n\nprint('#######series合并########')\nprint(pd.concat([ser1,ser2,ser3],axis=1,join='inner'))\n展示：\n#######series合并########\n    0   1   2\n0  17  13  13\n1  18  15  16\n2  11  12  13\n\n\n\n注意点：\n1、多个series对象进行合并的时候，需要用【】括起来\n2、默认按照列来合并，axis-1，按照行索引来排序，如果索引对应的值没有，用nan值填充\n3、这个默认是outer,说明是并集\n</code></pre>\n<h4>19.2.3 dataframe对象的合并</h4>\n<pre><code>df1 = pd.DataFrame(\n    {'key1':list('abcdefgh'),'data1':np.random.randint(-5,10,8)}\n)\n\ndf2 = pd.DataFrame(\n    {'key2':list('abcde'),'data2':np.random.randint(-5,10,5)}\n)\n\nprint(pd.concat((df1,df2)))\n\n展示：\n   data1  data2 key1 key2\n0    8.0    NaN    a  NaN\n1   -5.0    NaN    b  NaN\n2    7.0    NaN    c  NaN\n3   -2.0    NaN    d  NaN\n4   -2.0    NaN    e  NaN\n5    1.0    NaN    f  NaN\n6    6.0    NaN    g  NaN\n7   -3.0    NaN    h  NaN\n0    NaN    5.0  NaN    a\n1    NaN    2.0  NaN    b\n2    NaN    8.0  NaN    c\n3    NaN    3.0  NaN    d\n4    NaN    7.0  NaN \n\nprint(pd.concat((df1,df2),axis=1))\n展示：\n   data1 key1  data2 key2\n0     -5    a    5.0    a\n1      9    b   -1.0    b\n2      6    c    2.0    c\n3     -4    d    4.0    d\n4      0    e   -2.0    e\n5     -5    f    NaN  NaN\n6      4    g    NaN  NaN\n7      2    h    NaN  NaN\n\n\n\nprint(pd.concat((df1,df2),axis=1,join='inner'))\n展示：\n   data1 key1  data2 key2\n0      7    a      0    a\n1      0    b     -2    b\n2      0    c      1    c\n3      6    d      4    d\n4      3    e      5    e\n</code></pre>\n<h2>20、分组</h2>\n<p>###20.1 groupby</p>\n<pre><code>\n    def groupby(self, by=None, axis=0, level=None, as_index=True, sort=True,\n                group_keys=True, squeeze=False, **kwargs):\n        &quot;&quot;&quot;\n        Group series using mapper (dict or key function, apply given function\n        to group, return result as series) or by a series of columns.\n\n        Parameters\n        ----------\n        by : mapping, function, str, or iterable\n            Used to determine the groups for the groupby.\n            If ``by`` is a function, it's called on each value of the object's\n            index. If a dict or Series is passed, the Series or dict VALUES\n            will be used to determine the groups (the Series' values are first\n            aligned; see ``.align()`` method). If an ndarray is passed, the\n            values are used as-is determine the groups. A str or list of strs\n            may be passed to group by the columns in ``self``\n        axis : int, default 0\n        level : int, level name, or sequence of such, default None\n            If the axis is a MultiIndex (hierarchical), group by a particular\n            level or levels\n        as_index : boolean, default True\n            For aggregated output, return object with group labels as the\n            index. Only relevant for DataFrame input. as_index=False is\n            effectively &quot;SQL-style&quot; grouped output\n        sort : boolean, default True\n            Sort group keys. Get better performance by turning this off.\n            Note this does not influence the order of observations within each\n            group.  groupby preserves the order of rows within each group.\n        group_keys : boolean, default True\n            When calling apply, add group keys to index to identify pieces\n        squeeze : boolean, default False\n            reduce the dimensionality of the return type if possible,\n            otherwise return a consistent type\n</code></pre>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf1 = pd.DataFrame(\n    {'key1':list('abcdefgh'),'data1':np.random.randint(-5,10,8),'key2':list('11223344'),'data2':np.random.randint(-5,10,8)}\n)\n\nprint(df1)\n\n\nprint(group_obj)\nprint(group_obj.sum())\n求和\nprint(group_obj.mean())\n求均值\nprint(group_obj.size())\n求每一个分组的元素个数\n\n\n展示：\n   data1  data2 key1 key2\n0      4     -5    a    1\n1     -1      3    b    1\n2      0     -4    c    2\n3     -4     -1    d    2\n4      4     -1    e    3\n5     -3     -4    f    3\n6     -2      0    g    4\n7     -4     -5    h    4\n&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000004D547F0&gt;\n      data1  data2\nkey2              \n1         3     -2\n2        -4     -5\n3         1     -5\n4        -6     -5\n\n注意点：\n1、分组完成之后，返回的是一个分组对象\n2、分组之后，可以使用函数进行下一步的处理\n3、进行计算的过程中，非数字的数据不用参与计算\n</code></pre>\n<h3>20.1 分组之后的数据展示</h3>\n<pre><code>for name,data in group_obj:\n    print(name)\n    print('***********')\n    print(data)\n    print(type(data))\n    print('###########################')\n    \n 展示：\n &lt;pandas.core.groupby.DataFrameGroupBy object at 0x00000000027AAA20&gt;\n1\n***********\n   data1  data2 key1 key2\n0      2      5    a    1\n1     -4      8    b    1\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n###########################\n2\n***********\n   data1  data2 key1 key2\n2      4     -3    c    2\n3      5      5    d    2\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n###########################\n3\n***********\n   data1  data2 key1 key2\n4     -4      4    e    3\n5      0      9    f    3\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n###########################\n4\n***********\n   data1  data2 key1 key2\n6      6      0    g    4\n7      8     -4    h    4\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n###########################\n\n注意点：\n1、当迭代分组对象的时候，每一个元素的形式(分组名,属于该分组的数据)\n2、属于该分组的数据，也是dataframe对象\n</code></pre>\n<h3>20.2 分组对象转Python数据类型</h3>\n<pre><code>print(list(group_obj))\nprint(dict(list(group_obj)))\n展示：\n[('1',    data1  data2 key1 key2\n0      0     -3    a    1\n1     -1     -3    b    1), ('2',    data1  data2 key1 key2\n2      8     -4    c    2\n3      1      2    d    2), ('3',    data1  data2 key1 key2\n4      0     -2    e    3\n5      6     -4    f    3), ('4',    data1  data2 key1 key2\n6     -4      5    g    4\n7     -1      3    h    4)]\n{'1':    data1  data2 key1 key2\n0      0     -3    a    1\n1     -1     -3    b    1, '2':    data1  data2 key1 key2\n2      8     -4    c    2\n3      1      2    d    2, '3':    data1  data2 key1 key2\n4      0     -2    e    3\n5      6     -4    f    3, '4':    data1  data2 key1 key2\n6     -4      5    g    4\n7     -1      3    h    4}\n</code></pre>\n<h3>20.3 聚合</h3>\n<p>数组产生标量的过程</p>\n<p>常常用于分组之后的数据计算</p>\n<h4>20.3.1 内置的聚合函数</h4>\n<pre><code>sum() 求和\nmean（）均值\nmax 最大值\nmin 最小值\ncount（）\nsize（）\nprint(group_obj.describe())\n\n\n     data1                                           data2                 \\\n     count mean       std  min   25%  50%   75%  max count mean       std   \nkey2                                                                        \n1      2.0  5.5  4.949747  2.0  3.75  5.5  7.25  9.0   2.0  7.0  0.000000   \n2      2.0  4.5  0.707107  4.0  4.25  4.5  4.75  5.0   2.0  3.5  2.121320   \n3      2.0 -0.5  6.363961 -5.0 -2.75 -0.5  1.75  4.0   2.0  7.0  1.414214   \n4      2.0  4.0  5.656854  0.0  2.00  4.0  6.00  8.0   2.0  2.5  6.363961   \n\n                                 \n      min   25%  50%   75%  max  \nkey2                             \n1     7.0  7.00  7.0  7.00  7.0  \n2     2.0  2.75  3.5  4.25  5.0  \n3     6.0  6.50  7.0  7.50  8.0  \n4    -2.0  0.25  2.5  4.75  7.0  \n</code></pre>\n<h4>20.3.2 agg() 自定义函数做聚合运算</h4>\n<pre><code>print(group_obj.agg('max'))\nprint(group_obj.agg('min'))\n\n\n#使用多个内置的聚合函数，需要使用列表的形式，将字符串全部写入\nprint(group_obj.agg(['max','min','mean']))\n展示：\n     data1          data2         \n       max min mean   max min mean\nkey2                              \n1        9  -1  4.0     7   7  7.0\n2        5  -4  0.5     5   2  3.5\n3        9  -1  4.0     4   1  2.5\n4       -1  -4 -2.5     9   0  4.5\n\n#使用内置的聚合函数起中文名\nprint(group_obj.agg([('最大值','max'),('最小值','min'),('均值','mean')]))\n展示：\n     data1          data2         \n       最大值 最小值   均值   最大值 最小值   均值\nkey2                              \n1        5   1  3.0    -1  -3 -2.0\n2        9   3  6.0     9  -5  2.0\n3        3   2  2.5     5   1  3.0\n4        6  -2  2.0     4   3  3.5\n\n\n注意点：\n1、agg()填充的是函数，如果需要使用内置的聚合函数，用字符串的形式填写\n2、如果使用内置的函数起在别名的时候，注意（别名，字符串形式的内置函数名）\n</code></pre>\n<pre><code>def func(x):\n    # print(x)\n    ret = x.max()-x.min()\n    return ret\n    print('***********************')\n\nprint(group_obj.agg([('差值',func)]))\n\n自定义函数\nprint(group_obj.agg(\n    {\n        'data1':['max'],\n        'data2':[('最小值','min')]\n    }\n))\n\n\n不同的列用不同的聚合函数\n     data1 data2\n       max   最小值\nkey2            \n1        3    -2\n2        9    -5\n3        1     5\n4        8     0\n\n</code></pre>\n<h2>21、分组聚合后数据的处理</h2>\n<h3>21.1 先分组，再计算，添加前缀，merge关联</h3>\n<pre><code>dict_new = {\n    'data1':np.random.randint(5,15,8),\n    'data2':np.random.randint(5,15,8),\n    'key1':list('aabbccdd'),\n    'key2':['one','two','three','one','two','one','two','three'],\n    # 'data3':4\n}\n\ndf1 = pd.DataFrame(dict_new,index=list('ADCBEFGH'))\nprint(df1)\n\n#按照key1进行分组\ngroup_obj = df1.groupby('key1')\n# for name,data in group_obj:\n#     print(name)\n#     print('###############')\n#     print(data)\n\n\n#分组之后，进行求和操作\nsum_data_group = group_obj.sum()\nprint(sum_data_group)\n\nsum_data_group = sum_data_group.add_prefix('group_key1_')\nprint(sum_data_group)\n\n#先用merge进行关联\nmerge_df = pd.merge(df1,sum_data_group,left_on='key1',right_index=True)\nprint('##########合并之后的数据####')\nprint(merge_df)\n\n展示：\nC:\\ProgramData\\Anaconda3\\python.exe C:/Users/Administrator/Desktop/python_new/数组/分组聚合之后数据的处理.py\n   data1  data2 key1   key2\nA     11     13    a    one\nD     12     12    a    two\nC      8      9    b  three\nB      8      6    b    one\nE     13      6    c    two\nF     13     14    c    one\nG      5     12    d    two\nH      7      9    d  three\n      data1  data2\nkey1              \na        23     25\nb        16     15\nc        26     20\nd        12     21\n      group_key1_data1  group_key1_data2\nkey1                                    \na                   23                25\nb                   16                15\nc                   26                20\nd                   12                21\n##########合并之后的数据####\n   data1  data2 key1   key2  group_key1_data1  group_key1_data2\nA     11     13    a    one                23                25\nD     12     12    a    two                23                25\nC      8      9    b  three                16                15\nB      8      6    b    one                16                15\nE     13      6    c    two                26                20\nF     13     14    c    one                26                20\nG      5     12    d    two                12                21\nH      7      9    d  three                12                21\n注意点：\n1、在当前案例中，指定的连接条件：左表的列名，右表的行索引名\n</code></pre>\n<h3>21.2 transform</h3>\n<pre><code>group_obj = df1.loc[:,['data1','data2']].groupby(df1['key1'])\n# print(df1[['data1','data2']])\n\n# for name,data in group_obj:\n#     print(name)\n#     print('######')\n#     print(data)\n\nprint(group_obj.sum())\nprint('$$$$$$$$$$$$$$$$$$$$$')\ndata = group_obj.transform('sum').add_prefix('trans_group_')\nprint(data)\n\nret  = pd.concat([df1,data],axis=1)\nprint(ret)\n展示：\n   data1  data2 key1   key2\nA      6      9    a    one\nD     12     10    a    two\nC      6      5    b  three\nB     12     10    b    one\nE      7      6    c    two\nF     11      8    c    one\nG      9     13    d    two\nH     13     14    d  three\n      data1  data2\nkey1              \na        18     19\nb        18     15\nc        18     14\nd        22     27\n$$$$$$$$$$$$$$$$$$$$$\n   trans_group_data1  trans_group_data2\nA                 18                 19\nD                 18                 19\nC                 18                 15\nB                 18                 15\nE                 18                 14\nF                 18                 14\nG                 22                 27\nH                 22                 27\n   data1  data2 key1   key2  trans_group_data1  trans_group_data2\nA      6      9    a    one                 18                 19\nD     12     10    a    two                 18                 19\nC      6      5    b  three                 18                 15\nB     12     10    b    one                 18                 15\nE      7      6    c    two                 18                 14\nF     11      8    c    one                 18                 14\nG      9     13    d    two                 22                 27\nH     13     14    d  three                 22                 27\n\n注意点：\n1、transform 来计算的时候，会维持原来的数据结构\n2、df1.loc[:,['data1','data2']].groupby(df1['key1'])，选取数据其中的数据，按照原来数据中的列来进行分组\n</code></pre>\n<h2>22 、RFM模型</h2>\n<p>###22.1 定义：</p>\n<pre><code>RFM模型是衡量客户价值和客户创利能力的重要工具和手段。在众多的[客户关系管理](https://baike.baidu.com/item/%E5%AE%A2%E6%88%B7%E5%85%B3%E7%B3%BB%E7%AE%A1%E7%90%86)(CRM)的分析模式中，RFM模型是被广泛提到的。该机械模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱3项指标来描述该客户的价值状况。 \n\n\n根据美国数据库营销研究所Arthur Hughes的研究，客户数据库中有3个神奇的要素，这3个要素构成了数据分析最好的指标：\n最近一次消费 (Recency)\n消费频率 (Frequency)\n消费金额 (Monetary)\n\n</code></pre>\n<h3>22.2 案例</h3>\n<pre><code>import pandas as pd\nimport numpy as np\nimport time\n\ndata = pd.read_csv('RFM_TRAD_FLOW.csv',encoding='gbk')\n# print(data)\n\n# print(data.head())\ndata['time']=data['time'].map(lambda x:time.mktime(time.strptime(x,'%d%b%y:%H:%M:%S')))\n# print(data)、\ngroup_obj = data.groupby(['cumid','type'])\nR = group_obj[['time']].max()\n\n\n#转透视表\nr_trans = pd.pivot_table(R,index='cumid',columns='type',values='time')\n# print(r_trans)\n\nr_trans[['Special_offer','returned_goods']] = r_trans[['Special_offer','returned_goods']].apply(lambda x:x.replace(np.nan,min(x)),axis=0)\n# print(r_trans)\n\nr_trans['r_max'] = r_trans[['Normal', 'Presented','Special_offer']].apply(lambda x:max(x),axis=1)\n# print(r_trans)\n\n\n#            F\n\nF = group_obj[['transID']].count()\nprint(F)\n\nf_trans = pd.pivot_table(F,index='cumid',columns='type',values='transID')\n# print(f_trans)\n#\nf_trans[['Special_offer','returned_goods']] = f_trans[['Special_offer','returned_goods']].fillna(0)\n# print(f_trans)\n\nf_trans['returned_goods'] = f_trans['returned_goods'].map(lambda x:-x)\n# print(f_trans)\nf_trans['f_total'] =f_trans.apply(lambda x:sum(x),axis=1)\nprint(f_trans)\n\n\n\n#            M\n\nM = group_obj[['amount']].sum()\n\nm_trans = pd.pivot_table(M,index='cumid',columns='type',values='amount')\n# print(f_trans)\n#\nm_trans[['Special_offer','returned_goods']] = f_trans[['Special_offer','returned_goods']].fillna(0)\n# print(f_trans)\n# print(f_trans)\nm_trans['m_total'] =m_trans.apply(lambda x:sum(x),axis=1)\nprint(m_trans)\n\n### 合并\nRFM = pd.concat([r_trans['r_max'],f_trans['f_total'],m_trans['m_total']],axis=1)\nprint('##################')\nprint(RFM)\n\nRFM['r_score' ]= pd.cut(RFM.r_max,3,labels=[0,1,2])\nRFM['f_score' ]= pd.cut(RFM.f_total,3,labels=[0,1,2])\nRFM['m_score'] = pd.cut(RFM.m_total,3,labels=[0,1,2])\n\nprint(RFM)\n\ndef rfm_label(x,y,z):\n    if x==2 and y==2 and z==2:\n        return '重要价值客户'\n    elif x==2 and (y in [0,1]) and z ==2:\n        return '重要发展客户'\n    elif (x in [0,1]) and y==2 and z==2:\n        return '重要保持客户'\n    elif (x in [0,1] ) and (y in [0,1]) and z ==2:\n        return '重要挽留客户'\n    elif x==2 and y==2 and (z in [0,1]):\n        return '一般价值客户'\n    elif x ==2 and (y in [0,1] and (z in [0,1])):\n        return '一般发展客户'\n    elif (x in [0,1]) and y==2 and (z in [0,1]):\n        return '一般保持客户'\n    elif (x in [0,1]) and (y in [0,1]) and (z in (0,1)):\n        return '一般挽留客户'\n\n\nRFM['标签']=RFM[[ 'r_score', 'f_score', 'm_score']].apply(lambda x:rfm_label(x[0],x[1],x[2]),axis=1)\nprint(RFM)\n\n#选择满足条件的行\n# pd.read_sql_query()\n</code></pre>\n<h2>23 、pandas中sql语句</h2>\n<h3>23.1 基于第三方</h3>\n<p>sqlite3</p>\n<p>sqlalchemy</p>\n<h3>23.2 sqlite3使用案例</h3>\n<pre><code>import sqlite3\nimport pandas as pd\n\nsale = pd.read_csv('sale.csv',encoding='gbk')\nprint(sale)\n\n\n#使用特定的名称在内存中创建一个数据库\ncon = sqlite3.connect(':memory:')\nprint(con)\n\n#将dataframe对象注册成可sql查询的表\nsale.to_sql('sale',con)\n\n#查询出单独的一列\nyear_data = pd.read_sql_query('select  year from sale',con)\nyear_data_1 = pd.read_sql_query('select DISTINCT year from sale',con)\n\nprint(year_data_1)\n\n\n展示：\n    year market   sale  profit\n0   2010      东  33912    2641\n1   2010      南  32246    2699\n2   2010      西  34792    2574\n3   2010      北  31884    2673\n4   2011      东  31651    2437\n5   2011      南  30572    2853\n6   2011      西  34175    2877\n7   2011      北  30555    2749\n8   2012      东  31619    2106\n9   2012      南  32443    3124\n10  2012      西  32103    2593\n11  2012      北  31744    2962\n&lt;sqlite3.Connection object at 0x00000000025F7AB0&gt;\n   year\n0  2010\n1  2011\n2  2012\n</code></pre>\n<h3>23.3 sqlalchemy使用案例</h3>\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+pymysql://root:mysql@localhost/new_db?charset=utf8')\n\nsale = pd.read_csv('sale.csv',encoding='gbk')\n\n#将dataframe对象填写进入数据库\nsale.to_sql('sale',engine)\n</code></pre>\n<h2>24、绘图</h2>\n<pre><code>%matplotlib inline\n在jupyter notebook里面，注意\n\nplt不支持中文，所以要修改字体\nplt.rcParams['font.sans-serif'] = ['SimHei']\n\n#修改字体的大小\nplt.rcParams['font.size'] = 24\n\nx\\y轴的标签，可以设置字体的大小\nplt.xlabel('数据',fontsize=18)\nplt.ylabel('值',fontsize=18)\n\n# 去除-号，当字体是中文的时候，负号显示不出来\nplt.rcParams['axes.unicode_minus'] = False\n\n#增加图例\nplt.legend()\n#显示网格\nplt.grid()\n\n</code></pre>\n<h3>24.1创建画布</h3>\n<pre><code>def figure(num=None,  # autoincrement if None, else integer from 1-N\n           figsize=None,  # defaults to rc figure.figsize\n           dpi=None,  # defaults to rc figure.dpi\n           facecolor=None,  # defaults to rc figure.facecolor\n           edgecolor=None,  # defaults to rc figure.edgecolor\n           frameon=True,\n           FigureClass=Figure,\n           clear=False,\n           **kwargs\n           ):\n           \n 注意点：\n 1、dpi分辨率\n 2、figsize 指定画布的大小\n \n</code></pre>\n<h3>24.2 线型图</h3>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-comment\"># %matplotlib inline</span>\nfig = plt.figure()\nprint(fig)\n\n\narr1 = np.arange(<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">50</span>)\nprint(arr1)\n\nplt.plot(arr1)\n\nplt.savefig(<span class=\"hljs-string\">'a.png'</span>)\nplt.show()\n\nplt.plot(arr1,color=<span class=\"hljs-string\">'green'</span>,marker=<span class=\"hljs-string\">'*'</span>,markerfacecolor=<span class=\"hljs-string\">'red'</span>,markersize=<span class=\"hljs-number\">10</span>,linestyle=<span class=\"hljs-string\">'None'</span>)\nplt.show()\n\n注意点：\n<span class=\"hljs-number\">1</span>、plt.show()显示最后一次画布绘制的图形，显示完成之后，会清空内存中的数据\n</code></pre>\n<h3>24.3  add_subplot分割区域（绘制子图）</h3>\n<p>####24.3.1 第一种绘制子图的方式</p>\n<pre><code>def add_subplot(self, *args, **kwargs):\n\n\n\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\narr1 = np.random.randint(10,20,20)\narr2 = np.random.randint(10,20,20)\narr3 = np.random.randint(10,20,20)\narr4 = np.random.randint(10,20,20)\n\nax1.plot(arr1,color='r')\nax2.plot(arr2,color='g')\nax3.plot(arr3,color='purple')\nax4.plot(arr4,color='y')\n</code></pre>\n<h4>24.3.2 第二种子图的绘制方式</h4>\n<pre><code>fig.add_subplot(2,3,1)\nplt.plot(arr1)\n# plt.show()\n\nfig.add_subplot(232)\nplt.plot(arr2)\n\n\nfig.add_subplot(235)\nplt.plot(arr2)\n\nplt.show() \n</code></pre>\n<h3>24.4 直方图</h3>\n<pre><code>def hist(x, bins=None, range=None, density=None, weights=None, cumulative=False,\n         bottom=None, histtype='bar', align='mid', orientation='vertical',\n         rwidth=None, log=False, color=None, label=None, stacked=False,\n         normed=None, hold=None, data=None, **kwargs):\n注意点：\n1、bins是直方图特有的参数，表示的是直方的个数\n2、alpha是指透明度\n\n\nplt.hist(np.random.randint(10,100,40),bins=40,color='r',alpha=0.1)\nplt.show()\n\n</code></pre>\n<h3>24.5 散点图</h3>\n<pre><code>def scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None,\n            vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None,\n            hold=None, data=None, **kwargs):\n    ax = gca()\n    \nx = np.random.randint(10,200,50)\ny = x + 50 * np.random.rand(50)\nprint(x)\nprint(y)\nplt.scatter(x,y,color='g')\nplt.show()\n</code></pre>\n<p>###24.6 柱形图</p>\n<pre><code>x = np.arange(5)\n# print(np.array(range(5)))\nprint(x)\ny1 = np.random.randint(10,100,5)\ny2 = np.random.randint(10,100,5)\n\n\nplt.bar(x,y1,0.25,color='red')\nplt.bar(x+0.25,y2,0.25,color='green')\n\n\nplt.show()\n\n注意点：\n1、第一个参数是X轴的刻度\n2、第二个参数是Y轴的刻度\n3、第三个参数是柱形图的宽度\n</code></pre>\n<h3>24.7 混淆矩阵(表示二维数据的分布情况)</h3>\n<p>监督学习：混淆矩阵</p>\n<p>非监督学习：匹配矩阵</p>\n<pre><code class=\"language-python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">imshow</span><span class=\"hljs-params\">(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None,\n           vmin=None, vmax=None, origin=None, extent=None, shape=None,\n           filternorm=<span class=\"hljs-number\">1</span>, filterrad=<span class=\"hljs-number\">4.0</span>, imlim=None, resample=None, url=None,\n           hold=None, data=None, **kwargs)</span>:</span>\n    ax = gca()\n    \narr1 = np.random.randint(<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">50</span>,(<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">10</span>))\nprint(arr1)\nplt.imshow(arr1,cmap=plt.cm.Blues)\n\nplt.colorbar()\nplt.show()\n注意点：\n<span class=\"hljs-number\">1</span>、cmap指定颜色风格\n\n</code></pre>\n<p>###24.8 饼图（展示百分比的情况）</p>\n<pre><code>def pie(x, explode=None, labels=None, colors=None, autopct=None,\n        pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=None,\n        radius=None, counterclock=True, wedgeprops=None, textprops=None,\n        center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None):\n    ax = gca()\n    \n arr10 = np.random.randint(10,20,6)\nplt.pie(arr10,shadow=True,autopct='%2.2f%%',explode=[1,0,0,0,0,0],labels=['一月','2','3','4','5','6'])\n\nplt.show()\n\n\n注意点：\n1、labels的长度要和数据一致\n</code></pre>\n<h2>25、数理统计技术</h2>\n<p>右偏时一般算术平均数&gt;中位数&gt;众数，左偏时相反，即众数&gt;中位数&gt;平均数。正态分布三者相等。</p>\n<h3>25.1 客户的标签</h3>\n<p>分类为：基础标签，统计标签，模型标签</p>\n<p>基础的客户标签：性别、年龄、职业，可以从原始数据中直接获取的</p>\n<p>统计标签：原始数据的汇总得到的，比如客户的价值标签</p>\n<p>模型标签：基础标签、统计标签和已经存在的模型经过构建数据挖掘模型得到的，比如说客户的流失概率、违约概率</p>\n<h3>25.2 RFM模型</h3>\n<p>R：代表的是最后一次消费的时间（最新消费时间），可以代表用户的流失可能性，消费时间越久远，说明顾客的流失可能性越高</p>\n<p>F：代表一段时间内消费的频次</p>\n<p>M：一段时间内消费的总金额，可以代表顾客的价值</p>\n<h3>25.3  描述性统计分析</h3>\n<p>从总体数据中提炼变量的主要信息，完成业务分析报告</p>\n<p><img src=\"%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%901\" alt=\"1565666545636\"></p>\n<h3>25.4  统计推断与统计建模</h3>\n<h4>25.4.1 预测性</h4>\n<p><img src=\"%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%902\" alt=\"1565667077050\"></p>\n<h4>25.4.2 描述性</h4>\n<p><img src=\"%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%903\" alt=\"1565667156149\"></p>\n<h2>26、数据挖掘的技术与方法</h2>\n<h2>27、描述性数据挖掘算法</h2>\n<h3>27.1 聚类分析：客户细分</h3>\n<h3>27.2 关联规则分析</h3>\n<h2>28、预测性数据挖掘</h2>\n<h3>21.1 决策树</h3>\n<h3>21.2 knn算法</h3>\n<h3>21.3 logistic回归</h3>\n<h3>21.4 神经网络</h3>\n<h3>21.5 支持向量机</h3>\n<h3>21.6 集成学习</h3>\n<h3>21.7 预测类模型</h3>\n<h2>29、scikit-learn与机器学习</h2>\n<pre><code>&lt;https://sklearn.apachecn.org/docs/0.19.x/68.html&gt; \n</code></pre>\n<h2>30、机器学习的基础知识点</h2>\n<h3>30.1 特征值</h3>\n<h3>30.2 目标值</h3>\n<h3>30.3 监督学习与非监督学习</h3>\n<pre><code>    训练集(training set/data)/训练样例（training examples): 用来进行训练，也就是产生模型或者算法的数据集\n\n    测试集(testing set/data)/测试样例 (testing examples)：用来专门进行测试已经学习好的模型或者算法的数据集\n\n一般情况下，训练集与测试集的比例一般是7:3,6:4,8:2\n</code></pre>\n<h3>30.4 机器学习的步骤</h3>\n<pre><code>     8.1 把数据拆分为训练集和测试集\n\n     8.2 用训练集和训练集的特征向量来训练算法\n\n     8.2 用学习来的算法运用在测试集上来评估算法 （可能要设计到调整参数（parameter tuning), 用验证集（validation set）\n</code></pre>\n<h3>30.5 监督学习的分类与算法</h3>\n<pre><code>（1）分类\n\n决策树   银行信用自动评估系统\n\n临近取样  人脸识别\n\n支持向量机  图片中的红眼检测 结肠癌早晚期分类\n\n神经网络算法 手写数字识别 图片分类\n\n（2）回归\n\n线性回归 销售预测 价格预测\n\n零线性回归\n</code></pre>\n<h4>30.6 数据的类型</h4>\n<pre><code>按照机器学习的数据分类\n\n标称型：标称型目标变量的结果只在有限目标值中取值，比如说：去、不去（用于分类）\n\n数值型：数值型的目标变量的结果在无限的数值集合中取值，比如说：100.1 ，10000.001（用于回归分析）\n\n按照数据本身的分布特性\n\n离散型：没有规律，数值只能用自然数或者整数单位计算的，比如公司人数、进球个数或者某个类别\n\n连续型：存在规律，在指定的区间内，可以是任何一个数值，比如电影的票房，叶片的大小\n</code></pre>\n<h2>31、 特征工程</h2>\n<pre><code>更好的特征意味着更好的结果\n\n更好的特征意味着更简单的模型\n\n更好的特征意味这更强的鲁棒性\n</code></pre>\n<h3>31.1 特征预处理</h3>\n<pre><code>单个特征：\n\t归一化、标准化、缺失值处理\n\n多个特征：\n\t降维：PCA降维\n</code></pre>\n<h3>31.2 特征提取</h3>\n<p>现实生活中，存在很多不是连续变量，比如文字、图片，但是我们需要对他进行数学化表述，用到特征提取</p>\n<h3>31.3 分类特征变量的提取</h3>\n<h4>31.3.1 字典特征提取</h4>\n<pre><code>from sklearn import feature_extraction\nfrom sklearn.feature_extraction import DictVectorizer\n\ndata = [{'hair':'黑色','country':'中国'},{'hair':'棕色','country':'英国'},{'hair':'黑色','country':'西班牙'}]\n\ndef old():\n    dv = DictVectorizer()\n\n    #如果DictVectorizer()，没有进行任何设置使用默认值，转换出来的结果是一个稀疏矩阵\n    ret = dv.fit_transform(data)\n    print(ret)\n    print(dv.get_feature_names())\n    print(ret.toarray())\n    #稀疏矩阵可以使用toarray的方式转化成数组\n\ndef new():\n    dv = DictVectorizer(sparse=False)\n    #sparse=False代表直接转成数组\n    ret = dv.fit_transform(data)\n   \n    print(ret)\n    print(dv.inverse_transform(ret))\n    #转化成原始数据\n\nif __name__ == '__main__':\n    new()\n</code></pre>\n<h4>31.3.2 文本特征提取</h4>\n<pre><code>def text_handler():\n    # 文本数据分析\n    str1 = 'i love python'\n    str2 = 'good good study,day day up'\n\n    # 创建词频统计对象\n    # cv = CountVectorizer(stop_words='english')\n    cv = CountVectorizer(stop_words=['good'])\n\t#CountVectorizer统计词语出现的个数\n    ret = cv.fit_transform([str1,str2])\n    print(cv.get_feature_names())\n    print(ret.toarray())\n\n    print(ret)\n    pass\n    \n注意点：\n1、stop_words表示的是停用词，english表示使用英文停用词，如果需要自己指定，我们可以使用列表的方式写入\n2、一般情况下，我们直接使用sparse矩阵，因为语料库非常庞大，\n3、fit_transform填写可迭代对象\n\ndef text_handler_tf():\n    # 文本数据分析\n    str1 = 'i love python'\n    str2 = 'scikit-learn 是基于 Python 语言的机器学习工具。简单高效的数据挖掘和数据分析工具,可供大家在各种环境中重复使用'\n    new_str = cutword(str2)\n    print(new_str)\n\n    tv = TfidfVectorizer(stop_words=['大家'])\n    data = tv.fit_transform([str1,new_str])\n    print(tv.get_feature_names())\n    print(data.toarray())\n    \n    当前的主旨：如果一个词在当前文章中出现的概率较高，但是在其他文件中出现的很少，所以，这个词能够代表文章主旨\n\n</code></pre>\n<p>中文分词工具</p>\n<p>jieba,SnowNLP,NLPIR</p>\n<h4>31.3.3 jieba分词</h4>\n<pre><code class=\"language-：\">模式：\n\n全模式：将句子中所有的词组全部扫描出来\n\n精确模式：cut_all=False,适用于文本分析\n\n搜索引擎模式：搜索引擎分词\n</code></pre>\n<h3>31.4 数据的特征预处理</h3>\n<h4>31.4.1 归一化</h4>\n<p>出现问题：</p>\n<p>在特征（维度）较多的时候，可以预防某一维或者几维对于数据的影响过大。例如身高170，体重50kg,所以将不同来源的数据归类到同一个参考区间下</p>\n<p>常用方法：</p>\n<p>（1）对于比较常见的方法，是使用线性转换将数据映射到[0,1]之间的区间，变换的函数是</p>\n<p><img src=\"C:%5CUsers%5CAdministrator%5CDesktop%5Cpython-%5CPAI0408%5Cspark%5C1565831654068.png\" alt=\"1565831654068\"></p>\n<p>min：代表的是整个数据中的最小值</p>\n<p>max：代表的是整个数据中的最大值</p>\n<p>min和max的值，容易收到异常点的影响，并且在数据流的情况下，最大值和最小值是不断变化的</p>\n<p>适用情况：适用于精确的小数据场景下</p>\n<pre><code>def minmax():\n    print('########原始数据#########')\n    print(data_old)\n\n    minmax = MinMaxScaler()\n\n    data = minmax.fit_transform(data_old)\n    print('######转换之后的数据######')\n    print(data)\n</code></pre>\n<h4>31.4.2 标准化</h4>\n<p>经过处理之后，数据的均值为0，标准差1，处理函数：</p>\n<p><img src=\"C:%5CUsers%5CAdministrator%5CDesktop%5Cpython-%5CPAI0408%5Cspark%5C1565834179872.png\" alt=\"1565834179872\"></p>\n<p>适用于比较稳定，嘈杂的数据场景</p>\n<p>μ代表的是样本的平均值</p>\n<p>σ 代表样本的标准差</p>\n<p>将数据处理成符合正态分布的数据</p>\n<pre><code>def standar(dataset):\n    ss = StandardScaler()\n    data_new = ss.fit_transform(dataset)\n    return data_new\n</code></pre>\n<h4>31.4.3 异常缺失值处理</h4>\n<pre><code>strategy : string, optional (default=&quot;mean&quot;)\n    The imputation strategy.\n\n    - If &quot;mean&quot;, then replace missing values using the mean along\n      the axis.\n    - If &quot;median&quot;, then replace missing values using the median along\n      the axis.\n    - If &quot;most_frequent&quot;, then replace missing using the most frequent\n      value along the axis.\n      \n 填充策略\n \n \n def handler_nan():\n    data = [[1,3],[np.nan,7],[9,11]]\n    im = Imputer()\n    data_new = im.fit_transform(data)\n    print(data_new)\n    \n 注意点：\n 1、空值填充三种：均值，中位数，众数\n 2、axis=0代表的是列，axis=1 代表行\n</code></pre>\n<h4>31.4.4 降维</h4>\n<p>主成分分析：PCA</p>\n<pre><code>PCA致力解决的问题:\n\n第一，降维可以缓解维度灾难问题。\n\n第二，降维可以在压缩数据的同时让信息损失最小化。\n\n第三，理解几百个维度的数据结构很困难，两三个维度的数据通过可视化更容易理解。\n</code></pre>\n<p><img src=\"C:%5CUsers%5CAdministrator%5CDesktop%5Cpython-%5CPAI0408%5Cspark%5C1565837990960.png\" alt=\"1565837990960\"></p>\n<p>数据的降维，必然带来数据的损失</p>\n<p>在降维之前保证每一个维度的重要性是等同的</p>\n<p><a href=\"https://blog.csdn.net/javastart/article/details/79748677\">https://blog.csdn.net/javastart/article/details/79748677</a></p>\n<pre><code>def __init__(self, n_components=None, copy=True, whiten=False,\n             svd_solver='auto', tol=0.0, iterated_power='auto',\n             random_state=None):\n             \nn_components：指定的是降维之后存在的特征的维度\nwhiten：白化，对降维之后的每一个特征进行归一化处理\n\n\ndef pca():\n    data = [[1,2,3,4],[3,4,5,6],[6,7,8,1]]\n    print('降维之前')\n    print(data)\n    pa = PCA(n_components=2)\n    data_new = pa.fit_transform(data)\n    print('#降维之后')\n    print(data_new)\n    \n 注意点：\n 1、如果没有指定降维的个数的化，在原始的基础上，降下一维\n 2、降维不是指的数组的维度个数，而是说明的特征值的个数\n\n</code></pre>\n<h4>31.4.5  特征选择</h4>\n<p>原因：</p>\n<p>1、冗余：特征的相关度很高，容易消耗计算性能</p>\n<p>2、噪声：部分特征对于最终的预测结果有负影响</p>\n<p>特征选择方法：</p>\n<p>1、filter过滤式VarianceThreshold（删除低方差数据）</p>\n<p>2、emdedded 嵌入式:正则化、决策树</p>\n<p>3、wrapper 包裹式</p>\n<p>特征选择的功能</p>\n<p>减少特征的数量，降维，减少过拟合</p>\n<pre><code>def vtd():\n    data = [[1, 0, 3, 4], [3, 4, 5, 0], [6, 7, 0, 1]]\n    print(data)\n    vt = VarianceThreshold(1)\n    ret = vt.fit_transform(data)\n    print(ret)\n</code></pre>\n<h3>31.5 数据收集</h3>\n<p>数据的来源：</p>\n<p>爬虫、买、实时数据、网上公开的数据集</p>\n<p>数据处理成指定的格式</p>\n<p>筛选数据</p>\n<p>输入模型，进行训练</p>\n<p>调优</p>\n<p>保存模型</p>\n<p>load开头的小数据集</p>\n<p>fetch开头的是大数据集</p>\n<p>make 本地数据集</p>\n<pre><code>from sklearn.datasets import *\ndata = load_iris()\nprint(data)\n#获取的特征名\nprint(data.feature_names)\n获取的特征数组\nprint(data.data)\n获取的目标值名字\nprint(data.target_names)\n获取的目标数组\nprint(data.target)\n\n注意点：\n加载本地数据集\n</code></pre>\n<pre><code>data = fetch_20newsgroups()\nprint(data)\n注意点：联网下载大数据集\n</code></pre>\n<pre><code>data = make_classification()\nprint(data[0])\nprint(data[1])\n\n生成本地的分类数据集\n</code></pre>\n<h2>32、模型的选择</h2>\n<pre><code>   有监督学习(supervised learning)： 训练集有类别标记(class label)\n\n   无监督学习(unsupervised learning)： 无类别标记(class label)\n\n   半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集\n</code></pre>\n<p>监督学习：</p>\n<p>分类 K-近邻   决策树  贝叶斯 逻辑回归 支持向量机</p>\n<p>回归 线性回归 岭回归</p>\n<p>标注  隐马尔科夫（HMM）</p>\n<p>无监督：</p>\n<p>聚类</p>\n<h2>33、决策树</h2>\n<p>###33.1 信息熵</p>\n<p>熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。</p>\n<p><a href=\"https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF/111163\">信息</a>是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少<a href=\"https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E9%87%8F/420401\">信息量</a>。</p>\n<p>直到1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。信息熵这个词是C．E．香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。</p>\n<p>信息熵的单位：比特bit</p>\n<p><img src=\"1565922680465.png\" alt=\"1565922680465\"></p>\n<p>例子：猜世界杯冠军，假如一无所知，猜多少次？</p>\n<h3>33.2 关于节点的选择，算法（ID3）</h3>\n<p><img src=\"1565924105387.png\" alt=\"1565924105387\"></p>\n<h3>33.3 案例实现</h3>\n<pre><code>import pandas\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# import csv\n# data = csv.reader('buy_computer.xlsx')\ndata = pandas.read_excel('buy_computer.xlsx')\n\nx = data.loc[:,['age','income','student','credit_rating']]\ny = data[['Class:buys_computer']]\n\nprint('#########特征集#######')\nprint(x)\nprint('###########目标集######')\nprint(y)\n\nx_dict  = x.to_dict(orient='records')\n\ndv = DictVectorizer()\n\nX = dv.fit_transform(x_dict).toarray()\nprint(dv.get_feature_names())\nprint(X)\n\n\n\nY = LabelBinarizer().fit_transform(y)\nprint(Y)\n\n#导入自带的决策树模型\ndtc = DecisionTreeClassifier(random_state=0)\n\n#带入数据进行训练\ndtc.fit(X,Y)\n# 生成模型，进行预测\n# y_predict = dtc.predict([[0. 0. 1. 0. 1. 1. 0. 0. 1. 0.]])\n\n#展示出决策树\n\ndot_data = tree.export_graphviz(feature_names=dv.get_feature_names(),decision_tree=dtc,out_file=None,rounded=True,filled=True,special_characters=True)\nprint(dot_data)\n\nimport pydotplus\n\ngra = pydotplus.graph_from_dot_data(dot_data)\nprint(gra)\n# gra.write_pdf('file.pdf')\n</code></pre>\n<h3>33.4 优缺点</h3>\n<p>​\t决策树的优点：     直观，便于理解，小规模数据集有效</p>\n<p>​\t决策树的缺点：     处理连续变量不好     类别较多时，错误增加的比较快     可规模性一般</p>\n<h3>33.5 算法选择</h3>\n<pre><code> C4.5:  Quinlan               Classification and Regression Trees\n\n (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)              \n\n 共同点：都是贪心算法，自上而下(Top-down approach)               \n\n区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain) \n</code></pre>\n<p>##34 、K近邻（knn）</p>\n<h3>34.1 超参数</h3>\n<p>k</p>\n<h3>34.2 k近邻的距离</h3>\n<p><img src=\"C:%5CUsers%5CAdministrator%5CDesktop%5Cpython-%5CPAI0408%5C1566183914206.png\" alt=\"1566183914206\"></p>\n<p>欧式距离</p>\n<p><img src=\"C:%5CUsers%5CADMINI~1%5CAppData%5CLocal%5CTemp%5C1566181950749.png\" alt=\"1566181950749\"></p>\n<h3>34.3 pandas的展示</h3>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n\n\ndataset =  pd.read_excel(<span class=\"hljs-string\">'./knn案例.xls'</span>)\nprint(dataset)\n\nfeature = dataset.iloc[:<span class=\"hljs-number\">-1</span>,[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>]]\nprint(feature)\n\nx_unknown,y_unknown = (<span class=\"hljs-number\">18</span>,<span class=\"hljs-number\">90</span>)\nfeature[<span class=\"hljs-string\">'new_x'</span>] = (feature[[<span class=\"hljs-string\">'fightnum'</span>]]-x_unknown)**<span class=\"hljs-number\">2</span>\nfeature[<span class=\"hljs-string\">'new_y'</span>] = (feature[[<span class=\"hljs-string\">'kissnum'</span>]]-y_unknown)**<span class=\"hljs-number\">2</span>\nfeature[<span class=\"hljs-string\">'distance'</span>] = (feature[<span class=\"hljs-string\">'new_x'</span>]+feature[<span class=\"hljs-string\">'new_y'</span>])**(<span class=\"hljs-number\">1</span>/<span class=\"hljs-number\">2</span>)\nprint(feature)\n</code></pre>\n<h3>34.4 sklearn</h3>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span> sklearn.datasets <span class=\"hljs-keyword\">import</span> load_iris\n<span class=\"hljs-keyword\">from</span> sklearn.model_selection <span class=\"hljs-keyword\">import</span> train_test_split\n<span class=\"hljs-keyword\">from</span> sklearn.neighbors <span class=\"hljs-keyword\">import</span> KNeighborsClassifier\ndata_set = load_iris()\n<span class=\"hljs-comment\"># print(data_set)</span>\n\nfeatures = data_set.data\nlables = data_set.target\nprint(features)\nprint(lables)\n\nX_train, X_test, y_train, y_test = train_test_split(features,lables,test_size=<span class=\"hljs-number\">0.23</span>)\n\n<span class=\"hljs-string\">'''\n X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, test_size=0.33, random_state=42)\n'''</span>\n\nknn_cls = KNeighborsClassifier()\nknn_cls.fit(X_train,y_train)\n\ny_pedict = knn_cls.predict(X_test)\nprint(<span class=\"hljs-string\">'这是原始测试集的目标值'</span>)\nprint(y_test)\nprint(<span class=\"hljs-string\">'预测数据'</span>)\nprint(y_pedict)\n\nscore = knn_cls.score(X_test,y_test)\nprint(<span class=\"hljs-string\">'得分'</span>)\nprint(score)\n</code></pre>\n<h3>34.5 分类器性能</h3>\n<table>\n<thead>\n<tr>\n<th></th>\n<th style=\"text-align:center\">正例</th>\n<th style=\"text-align:center\">假例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>正例</td>\n<td style=\"text-align:center\">真正例（TP）</td>\n<td style=\"text-align:center\">伪反例（FN）</td>\n</tr>\n<tr>\n<td>假例</td>\n<td style=\"text-align:center\">伪正例（FP）</td>\n<td style=\"text-align:center\">真反例（TN)</td>\n</tr>\n</tbody>\n</table>\n<p>精确率：预测结果为正例样本中真实为正例的比例（查的准）</p>\n<p>召回率：真实为正例的样本中预测结果为正例的比例（查的全）</p>\n<p>准确率 = 预测对的/所有 = （TP+TN）/(TP+FP+FN+TN)</p>\n<p>精确率 = TP/(TP+FP)</p>\n<p>召回率 = TP/(TP+FN)</p>\n<p>调和平均数 F1</p>\n<p><img src=\"C:%5CUsers%5CAdministrator%5CDesktop%5Cpython-%5CPAI0408%5C1566185408267.png\" alt=\"1566185408267\"></p>\n<p>如果精确率和召回率差距过大的模型，没有意义再使用，</p>\n<p>F1表示了模型的稳健性</p>\n<h3>34.6 K值对与模型的影响</h3>\n<p>K过小，容易收到噪声数据的影响</p>\n<p>K过大，容易收到数量的影响</p>\n<h3>34.7 优缺点</h3>\n<pre><code>优点：\n简单          \n易于理解          \n容易实现          \n通过对K的选择可具备丢噪音数据的健壮性\n\n缺点：\n需要大量空间储存所有已知实例          \n算法复杂度高（需要比较所有已知实例与要分类的实例）          \n当其样本分布不平衡时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大，但这个新的未知实例实际并木接近目标样本\n \n \n使用场景\n适用于小数据场景，几千--几万的数据量\n \n</code></pre>\n<h2>35、贝叶斯</h2>\n<h3>35.1 联合概率与条件概率</h3>\n<pre><code>联合概率：包含多个条件，且条件同时成立的概率\n\nP(A,B) = P(A)P(B)\n\n条件概率：事件A在另外一个事件B已经发生的条件下发生概率\n\nP(A|B) \n\n特性\n\nP(A1,A2|B) = P(A1|B)*P(A2|B)\n</code></pre>\n<h3>35.2 朴素贝叶斯公式</h3>\n<pre><code class=\"language-python\">P(C|W) = P(W|C)P(C)/P(W)\n\nC是指文档的类别\n\nW为给定文章的特征值\n\n\n\nP(C|F1，F2，F3........) = P(F1，F2，F3|C)P(C)/P(F1，F2，F3.......)\nP(C)是指每个文档类别的概率（文档的类别/总文档数）\nP(F1，F2，F3|C) = 在给定的类别情况下，特征出现的概率\nP(F1，F2，F3.......)预测文档中每一个词的概率\n</code></pre>\n<p>p(影院，支付宝，云计算|科技)*P（科技）=0.0045</p>\n<p>p(影院，支付宝，云计算|娱乐)*P（娱乐）=0</p>\n<h3>35.3 拉普拉斯平滑系数</h3>\n<p>出现的原因：</p>\n<p>在进行词频统计的时候，可能会出现多个词语出现次数为0的情况，算出来的该楼层可能为0，但是是不合理的</p>\n<p><img src=\"1566265533020.png\" alt=\"1566265533020\"></p>\n<p>a为指定的系数，一般为1，m为特征词出现的个数</p>\n<h3>35.4 优缺点</h3>\n<p>由古典的数学理论，所以有稳定的分类效率</p>\n<p>分类准确率较高，速度快</p>\n<p>适用于文本分析</p>\n<p>缺点：</p>\n<p>建立在样本属性相互独立的基础上，如果属性之间存在相关联的情况下，分类效果不好</p>\n<h2>36 、SVM</h2>\n<h3>36.1 过拟合与欠拟合的情况</h3>\n<p><img src=\"1566273494540.png\" alt=\"1566273494540\"></p>\n<h3>36.2 参数的问题</h3>\n<pre><code class=\"language-python\">kernel=<span class=\"hljs-string\">'linear'</span>时，为线性核，C越大分类效果越好，但有可能会过拟合（defaul C=<span class=\"hljs-number\">1</span>）。\n\nkernel=<span class=\"hljs-string\">'rbf'</span>时（default），为高斯核，gamma值越小，分类界面越连续；gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。\n\ndecision_function_shape=<span class=\"hljs-string\">'ovr'</span>时，为one v rest，即一个类别与其他类别进行划分，\n\ndecision_function_shape=<span class=\"hljs-string\">'ovo'</span>时，为one v one，即将类别两两之间进行划分，用二分类的方法模拟多分类的结果。\n</code></pre>\n<h2>37 、线性回归</h2>\n<h3>37.1 线性回归的基本介绍</h3>\n<p>简单线性回归(Simple Linear Regression)</p>\n<p>1 很多做决定过过程通常是根据两个或者多个变量之间的关系</p>\n<p>2 回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联</p>\n<p>3 被预测的变量叫做：因变量(dependent variable), y, 输出(output)</p>\n<p>4 被用来进行预测的变量叫做： 自变量(independent variable), x, 输入(input)</p>\n<h3>37.2 简单线性回归介绍</h3>\n<p>1 简单线性回归包含一个自变量(x)和一个因变量(y)</p>\n<p>2 以上两个变量的关系用一条直线来模拟</p>\n<p>3 如果包含两个以上的自变量，则称作多元回归分析(multiple regression)</p>\n<p><img src=\"../PAI0715/1566349142775.png\" alt=\"1566349142775\"></p>\n<p>简单线性回归方程</p>\n<p>E(y) = β0+β1x</p>\n<p>这个方程对应的图像是一条直线，称作回归线</p>\n<p>其中，</p>\n<p>β0是回归线的截距</p>\n<p>β1是回归线的斜率</p>\n<p>E(y)是在一个给定x值下y的期望值（均值）</p>\n<p>正向关系、负向关系、无关系</p>\n<p>β1 &gt;0            β1&lt;0          β1=0</p>\n<h3>37.3 多元线性回归</h3>\n<p><img src=\"1566353202701.png\" alt=\"1566353202701\"></p>\n<p><img src=\"1566353220985.png\" alt=\"1566353220985\"></p>\n<p><img src=\"1566353236813.png\" alt=\"1566353236813\"></p>\n<p><img src=\"1566353249770.png\" alt=\"1566353249770\"></p>\n<p>注意：特殊的是X0为1</p>\n<h3>37.4损失函数（误差大小）</h3>\n<p><img src=\"1566350634970.png\" alt=\"1566350634970\"></p>\n<p>损失函数：</p>\n<p><img src=\"../PAI0715/1566353135947.png\" alt=\"1566353135947\"></p>\n<p>是的损失函数是最小值：误差是最小的</p>\n<p>（1）使用梯度下降算法</p>\n<p>（2）正规方程（简单的线性回归）</p>\n<h3>37.5 正规方程</h3>\n<p><img src=\"1566353857419.png\" alt=\"1566353857419\"></p>\n<p>X是指特征值矩阵</p>\n<p>y目标值矩阵缺点：特征值过于复杂的时候，求解速度太慢了</p>\n<h3>37.6 梯度下降算法</h3>\n<p>以单变量为例</p>\n<p><img src=\"1566354140984.png\" alt=\"1566354140984\"></p>\n<p>a是指学习速率</p>\n<p><img src=\"1566354224691.png\" alt=\"1566354224691\"></p>\n<p>这个过程按照某一点在W1上的偏导数下降来寻找最低点</p>\n<p><img src=\"1566354299785.png\" alt=\"1566354299785\"></p>\n<p>函数的意义：沿着这个函数的下降的方向，找到山谷的最低点，不断的取更新w的值</p>\n<h3>37.7 均方误差（MSE）</h3>\n<p><img src=\"1566356717465.png\" alt=\"1566356717465\"></p>\n<pre><code>from sklearn.metrics import mean_squared_error\ndef mean_squared_error(y_true, y_pred,\n                       sample_weight=None,\n                       multioutput='uniform_average'):\n                       \n注意点：\n1、真实值与预测值为标准化之前的值\n2、返回值是一个浮点型数据\n\n</code></pre>\n<h3>37.7 梯度下降与正规方程的对比</h3>\n<table>\n<thead>\n<tr>\n<th>梯度下降</th>\n<th>正规方程</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>需要选择学习率</td>\n<td>不需要</td>\n</tr>\n<tr>\n<td>需要多次迭代</td>\n<td>一次运算就可以了</td>\n</tr>\n<tr>\n<td>当特征量很多的时候，适用情况很好</td>\n<td>需要计算，如果特征数量很大的化，计算比较大，因为矩阵逆的计算时间复杂度为o(n**3)</td>\n</tr>\n<tr>\n<td>适用与各种模型的计算</td>\n<td>只适合于线性模型，不适合逻辑回归模型</td>\n</tr>\n</tbody>\n</table>\n<p>小规模数据集 :正规方程</p>\n<p>大规模数据集：SGD</p>\n<h3>37.8 过拟合与欠拟合</h3>\n<p><img src=\"1566359465114.png\" alt=\"1566359465114\"></p>\n<table>\n<thead>\n<tr>\n<th>过拟合</th>\n<th>在训练集上获得更好的拟合，但是在训练集以外的数据集上表现很差</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>欠拟合</td>\n<td>在训练集上表现不好，在训练集以外的数据集上表现也不好</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"1566359599989.png\" alt=\"1566359599989\"></p>\n<p><img src=\"1566360058326.png\" alt=\"1566360058326\"></p>\n<h3>37.9 解决过拟合与欠拟合的问题</h3>\n<p>解决：</p>\n<p>欠拟合：</p>\n<p>原因：学习到的特征过少</p>\n<p>解决：增加特征数量</p>\n<p>过拟合：</p>\n<p>原因：原始输入的数据的特征过多，存在一些没有必要的嘈杂数据，模型之所以过于复杂，是因为模型想要更多的取匹配兼顾每一个数据点</p>\n<p>解决：</p>\n<p>1、进行特征的选择，消除一些关联数据较大的特征（最困难）</p>\n<p>2、交叉验证</p>\n<p>3、正则化</p>\n<h3>37.10 L2正则化</h3>\n<p><img src=\"image-20190822092741736.png\" alt=\"image-20190822092741736\"></p>\n<p>优点：</p>\n<p>参数越小，说明模型越简单，模型越简单就越不容易出现过拟合的现象</p>\n<p>###37.11 岭回归</p>\n<p>具有L2正则化的线性最小二乘法</p>\n<pre><code>from sklearn.linear_model import Ridge\n</code></pre>\n<h2>38、模型的调优</h2>\n<h3>38.1交叉验证</h3>\n<p>目的：为了让被评估的模型更加的准确</p>\n<p><img src=\"image-20190822094637580.png\" alt=\"image-20190822094637580\"></p>\n<h3>38.2 网格搜索</h3>\n<pre><code>from sklearn.model_selection import GridSearchCV\n</code></pre>\n",
  "link": "/zh-cn/blog/数据分析/数据分析.html",
  "meta": {}
}